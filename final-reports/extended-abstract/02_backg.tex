% A Theory section should extend, not repeat, the background to the
% article already dealt with in the Introduction and lay the foundation for further work.
\section{Optimization}
\label{sec:backg}
An optimization process consists in two main parts: the model of the problem to be optimized and the optimization algorithm. The creation of the model involves the definition of variables, objective functions, and, optionally, constraints. In architectural design optimization, the variables are the parameters whose value will be modified, objective functions represent the performance criteria that we aim to optimize, and constraints are the conditions on the parameters that must be satisfied so that a solution is valid for the defined problem.

Depending on the number of objectives, optimization problems can be classified as \ac{SOO} or \ac{MOO}. The former focuses on the optimization of one objective, whilst the latter focuses on more complex problems that involve multiple objectives.
Despite the increased complexity, \ac{MOO} encloses benefits, such as the visualization of the trade-offs among the different objectives and the selection of the solution that better fits the users' needs. This can be achieved through Pareto-based optimization, an approach that finds the solutions for which it is impossible to improve an objective value without deteriorating others. These solutions are called nondominated and represent the Pareto front. 

% Typically building design involves multiple conflicting objectives, such as maximum lighting comfort \textit{versus} maximum thermal comfort, or minimum energy consumption \textit{versus} maximum thermal comfort. When confronted with the optimal solutions returned by Pareto-based optimization, architects can compare the different design options according to different performance criteria, and, thus, make more informed decisions about the different compromises involved. However, this approach presents some drawbacks towards its application, namely, the requirement for a larger number of function evaluations and the difficulties in the visual representation of optimal solutions when the number of objectives is greater than three.

\subsection{Optimization Algorithms}
Optimization algorithms can be classified differently according to their properties. Considering the extent of the search, algorithms can be distinguished into local or global. Local optimization algorithms strive to find a solution for which the objective function yields a better value than for all the other solutions in its vicinity, i.e., a local optimum. These algorithms are usually highly sensitive to the starting point of the search and they tend to focus on smaller regions of the solution space. Conversely, global optimization algorithms strive to find globally optimal solutions, i.e., the best of all the local optima. To this end, these algorithms explore larger regions of the solution space, requiring a larger number of evaluations. However, this can be problematic for problems involving expensive objective functions. In these cases, one can apply a global algorithm to identify a promising region and then a local algorithm to more rapidly converge towards the optima within that region. 

A second classification differentiates deterministic and stochastic algorithms. Given the same starting point and configuration, deterministic algorithms systematically apply the same sequence of steps and, as a consequence, they always return the same result. In contrast, stochastic algorithms include some form of randomness and, therefore, often yield different results for the same starting point and algorithm's configuration.

The final distinction is between derivative-based and derivative-free algorithms, depending on their use of information about the objective functions' derivatives (e.g., the direction and magnitude of its greatest increase). 

In \ac{BPO}, the objective function tends to be simulation-based and, thus, does not have an analytical form. In these cases, it is not possible to apply derivative-based optimization algorithms. While, ideally, one could apply finite-difference algorithms to approximate the derivatives, this would require several extra simulations, which becomes impractical for problems involving expensive objective functions. Instead, one must resort to derivative-free optimization algorithms, which treat the objective functions as \textit{black-boxes} \cite{Rios2013}.

The following subsections describe three classes of derivative-free algorithms according to the classification proposed in the context of architectural design \cite{Wortmann2017ADO}, which first subdivides the algorithms according to the search strategy's determinism, namely, metaheuristics and iterative algorithms, and then partitions the latter into direct-search and model-based algorithms, based on the function explored to guide the search. 

\subsection{Direct-search Algorithms}
Direct-search algorithms iteratively evaluate a sequence of candidate solutions, proposed by a deterministic strategy, and select the best solution obtained up to that moment \cite{Conn2009}. They are regarded as valuable tools to address complex optimization problems, not only because most of them were proved to rely on solid mathematical principles, but also due to their good performance at initial stages of the search process \cite{Rios2013}. 

The main limitations of these algorithms are their performance deterioration with an increasing number of input variables and their slow asymptotic convergence rates as they get closer to the optimal solution \cite{Kolda2003}. Despite the existence of algorithms and benchmarks comparing \ac{SOO} direct-search algorithms \cite{Wortmann2017GABESTCHOICE,Waibel2018}, only recently have these started to appear in the context of \ac{MOO}~\cite{Custodio2010}.

Undoubtedly, one of the most relevant direct-search algorithms is the \textit{\ac{NMS}} \cite{Nelder1964}, a local \ac{SOO} direct-search algorithm. The \textit{\ac{NMS}} algorithm envelopes a region of the design space using a simplex, which is a generalization of a triangle to arbitrary dimensions, which is then successively modified using geometric operations that replace the simplex's worst vertex values.

Another interesting simplex-based algorithm is \textit{SUBPLEX} \cite{Rowan1990}, which attempts to overcome the \textit{\ac{NMS}} difficulties when addressing higher dimensional problems by decomposing the problem in low\nobreakdash-\hspace{0pt}dimensional
subspaces. The most promising ones are then explored by \textit{\ac{NMS}}.

Differently from the previous local algorithms, \textit{\ac{DIRECT}} \cite{Jones1993DIRECT} is a global \ac{SOO} algorithm, which recursively subdivides the design space into smaller multidimensional hyper-rectangles, each represented by a solution in their center. For each solution, the objective function is evaluated, thus yielding an estimate of the quality of each rectangle. Based on these values, \textit{\ac{DIRECT}} focus the search on more promising regions of the design space, further subdividing those. \textit{\ac{DIRECT}-L} \cite{Gablonsky2001} is a modification of \textit{\ac{DIRECT}} that minimizes the overall number of function evaluations, making it more efficient for functions with few local optima and a single global optimum. 

\textit{\ac{DMS}} \cite{Custodio2010} is a global multi-objective direct-search algorithmic framework which combines the main ideas of directional direct-search algorithms with Pareto dominance concepts. In simple terms, \textit{\ac{DMS}} maintains a list of feasible nondominated solutions and their associated step sizes, from which it iteratively evaluates a few solutions along a predefined set of directions located at a distance determined by the solution's step size. The list is updated when better solutions are found. Otherwise, the step size is reduced. The process then repeats. 

Overall, direct-search algorithms are not as popular as other classes of derivative-free algorithms. Nevertheless, their convergence proofs and the recent developments in the field of \ac{MOO} make this class very appealing for \ac{BPO}. 

\subsection{Metaheuristics Algorithms}
Metaheuristics are algorithms that rely on randomization, and biological or physical analogies to locate good solutions in complex design spaces \cite{Glover2003Metaheuristics}. Their non-deterministic and inexact nature confer them the ability to handle complex and irregular objective functions.

Metaheuristics can be good optimization algorithms when provided with sufficient amounts of time to do the necessary objective function evaluations \cite{Conn2009}. Unfortunately, this is not possible in architecture, where each evaluation can be a time-consuming simulation. However, because of their stochastic nature, limiting the number of evaluations has repercussions on the convergence guarantees \cite{Hasancebi2009}.

Depending on the metaphors adopted by each algorithm, metaheuristics can be classified in different subclasses, including, \acp{EA}, which explore natural selection and evolution concepts (e.g., reproduction, recombination, crossover, and mutation), Swarm algorithms, which explore collective intelligence through the cooperation of homogeneous agents in the environment (e.g., birds), and Physical Algorithms, which take inspiration from physical processes (e.g., the annealing process in metallurgy) \cite{Glover2003Metaheuristics}.

% GA Explained
The \textit{\ac{GA}} is an \ac{EA} that explores biological evolution to search for better solutions. It randomly generates an initial set of individuals representing the candidate solutions, called population, which is then evolved for a specified number of iterations (or generations). The evolution process is comprised of four main phases: (1) adaptability, where individuals of the population are assigned a suitability or fitness value; (2) mating selection, where pairs of individuals are selected for reproduction based on a probabilistic function; (3) crossover, where the variables' values of the selected individuals are recombined to produce new individuals; and (4) mutation, where new individuals are subjected to random copying errors with a certain probability. While in earlier generations, populations are usually more diverse, in final generations, their individuals are often similar to the fittest individuals, thus emulating the natural selection mechanism \cite{Glover2003Metaheuristics}.

% NSGA-II
Along the same line, \textit{\ac{NSGA-II}} is a \textit{\ac{GA}} specially tailored for \ac{MOO} problems, which incorporates the ideas of population genetics and evolution, Pareto dominance, and a crowding measure to attain an approximation of the Pareto front that is both accurate and diverse \cite{Deb2002}. In simple terms, \textit{\ac{NSGA-II}} maintains an archive and a population, which are iteratively evolved until a stopping criteria is met. Every iteration, the algorithm combines the archive and the population and sorts their individuals according to their nondomination ranks, a measure of the number of Pareto fronts that have to be removed until an individual becomes nondominated. Individuals belonging to the same rank are also sorted according to a density measure, based on the average distance of their two closest individuals. Afterwards, the archive is replaced with the best $50\%$ individuals and the next offspring produced based on the recombination and mutation of the individuals in the archive.

% PSO
Although less explored, \ac{PSO} algorithms have also been shown promising results in some optimization problems, surpassing widely reputed \acp{EA}. In their simpler versions, \ac{PSO} algorithms are global algorithms inspired by biological systems, such as the collective behavior of flocking birds, which interact and learn from one another to solve problems. In \ac{PSO}, the intelligence is decentralized, self-organized, and distributed throughout the participating particles (or swarm), which maintain information about their velocity, current and individual best positions, and also the global best position known to the swarm. At each time step, the position and velocity of each particle are updated according to the position of the best swarm or close neighbor \cite{Glover2003Metaheuristics}.
	
Overall, metaheuristics are stochastic algorithms, mostly without convergence guarantees, usually requiring several hundreds or even thousands of evaluations in order to obtain good solutions. For problems involving expensive objective functions, like most architectural optimization problems, and especially for \ac{MOO} problems, these algorithms are usually not a good choice. Unfortunately, there is a predominance of these algorithms (e.g., \textit{\ac{GA}} and \textit{\ac{NSGA-II}}) among \ac{BPO} practitioners \cite{Wortmann2017GABESTCHOICE}, with very few optimization tools supporting other classes of algorithms. Nevertheless, they can be very efficient solvers when properly configured. However, the optimal set of parameters is problem-specific and the same configuration applied to another problem might yield a bad performance.

\subsection{Model-based Algorithms}
Model-based algorithms replace the original objective functions with approximations. These approximations, called the surrogates, are generated from a limited set of known objective function values and can then be efficiently explored to determine the promising candidate solutions to evaluate next. These evaluations are then used to improve the surrogates and this process is repeated until a stopping condition is satisfied \cite{Koziel2011}. 

% Nowadays, the techniques applicable to the generation of surrogate models range from trust-region methods to \ac{ML} techniques. These techniques can be used to create (1) local surrogates, i.e., models where the approximation to the objective function is built around a certain point, and (2) global surrogates, i.e, models where the approximation is generated from all the obtained points. Whilst the former relies on the construction of simple, partial models of the objective function, the latter relies on the creation of a full model, which requires balancing the need for improving the model's accuracy by exploring broader regions in the solution space with the need for improving the objective function's value by exploiting promising regions~\cite{Koziel2011}. This balance is determined by a strategy that selects the next promising solutions to evaluate. 

Undoubtedly, the best feature of model-based algorithms is the reduction in total optimization time. This is particularly relevant in the context of \ac{BPO}, which involves time-consuming objective functions. Notwithstanding the existence of different studies involving \ac{ML} techniques for the creation of surrogate models~\cite{Koziel2011}, including \ac{MLP} and \textit{\ac{RBF}}, only a few have actually been applied in the context of architecture. This scenario is even more self-evident when we shift from the single- to the multi-objective optimization context.

Two relevant local model-based algorithms are \textit{\ac{COBYLA}} and \ac{BOBYQA}. The former uses a simplex to iteratively generate linear approximations of the objective function, whereas the latter generates quadratic approximations instead \cite{Powell1994COBYLA, Powell2009BOBYQA}.

\ac{RBF}-based algorithms create a global approximation of the objective function as the weighted sum of radial basis functions, whose weights are estimated based on the interpolation of previous evaluation results and are iteratively updated to improve the approximation.

When compared with metaheuristics, global model-based algorithms exhibit very good performance at initial stages of an optimization process, allowing for considerable speedups in the overall optimization.