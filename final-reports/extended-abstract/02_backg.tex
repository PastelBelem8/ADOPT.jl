% A Theory section should extend, not repeat, the background to the
% article already dealt with in the Introduction and lay the
% foundation for further work.

\section{Optimization Algorithms}
\label{sec:backg}

Optimization algorithms can be classified differently according to their properties. One important classification regards the extent of the search for optimal solutions, which can be global or local. Local optimization algorithms strive to find a locally optimal solution, i.e., for which the objective function yields a better value than for all the other solutions in its vicinity. Moreover, local algorithms are usually highly sensitive to the starting point of the search and they tend to focus on smaller regions of the solution space. In contrast, global optimization algorithms strive to find globally optimal solutions, i.e., the best of all the locally optimal solutions. To that end, these algorithms explore larger regions of the solution space and typically require several evaluations to find global optima. However, the increased number of evaluations can be problematic in problems involving expensive objective functions. To minimize this impact, a good approach might be to apply a global algorithm to identify a promising region and then apply a local algorithm to more rapidly find the optima within that region. 

A second classification differentiates deterministic and stochastic algorithms depending on the determinism of the algorithms' outcomes. Given the same starting point and configuration, deterministic algorithms systematically apply the same sequence of steps and, as a consequence, they always return the same result. In contrast, stochastic algorithms include some form of randomness within their description and, therefore, often yield different results for the same starting point and algorithm's configuration.

The third, and final, distinction is between derivative-based and derivative-free algorithms, which differ in the type of information used during the search. Derivative-based (or gradient-based) algorithms explore information from the derivatives of objective functions, i.e., the direction and magnitude of the greatest increase of the function, to guide the search. % Consequently, they solve problems explicitly defined through mathematical forms very efficiently, as the derivatives' information is easily available. 
However when the objective function's analytical form is unknown and the derivatives are unavailable, these algorithms cannot be applied. Although finite-difference methods could be applied to approximate the derivatives, these require several extra function evaluations, which becomes impractical for problems involving expensive objective functions. In these cases, it becomes necessary to resort to derivative-free algorithms, which, instead of exploiting information about derivatives, treat the objective functions as \textit{black-boxes} and use the result of previously evaluated solutions to guide the search~\cite{Rios2013}.

Derivative-free optimization algorithms, commonly known as black-box optimization algorithms within the architectural community \cite{Wortmann2016BBO},
can treat the results of performance simulations as the functions to optimize and, consequently, avoid the difficulty of deriving analytical formulas describing building performance \cite{Machairas2014}.

For the past decades, the constant development and improvement of derivative-free optimization resulted in the creation of algorithms with different underlying assumptions and different properties. Although there is no standardized classification for these optimization algorithms~\cite{Rios2013, Wortmann2017ADO}, it is possible to group them according to their main mechanisms and ideas. This dissertation follows the classification proposed in the context of architectural design~\cite{Wortmann2015AdvSBO}, which first subdivides the algorithms according to the search strategy's determinism, namely, metaheuristics and iterative methods, and only then proceeds to partition the latter into direct-search and model-based methods, based on the function explored to guide the search. Albeit the apparent chasm between these classifications, some algorithms draw ideas from distinct classes, thus emphasizing not only the blurred lines of such categorizations, but also the difficulties that lie within the definition of more standardized classifications. 

The following subsections describe the classes of algorithms that will be explored in this dissertation, including the three classes of derivative-free algorithms: direct-search, metaheuristics, and model-based. For each section, a summarized explanation about the most relevant algorithms will also be provided. 

\subsection{Direct-search Algorithms}
Although there seems to be no precise definition for direct-search algorithms, these are often identified as algorithms that iteratively: (1) evaluate a sequence of candidate solutions, proposed by a deterministic strategy; and (2) select the best solution obtained up to that time \cite{Conn2009}. They are regarded as valuable tools to address complex optimization problems, not only because most of them were proved to rely on solid mathematical principles, but also due to their good performance at initial stages of the search process~\cite{Rios2013}. 

The main limitations of these algorithms is their performance deterioration with the increase on the number of input variables and their slow asymptotic convergence rates as they get closer to the optimal solution~\cite{Kolda2003}. Despite the existence of algorithms and benchmarks comparing \ac{SOO} direct-search algorithms \cite{Wortmann2017GABESTCHOICE,Waibel2018}, only recently have these started to appear in the context of \ac{MOO}~\cite{Custodio2010,Custodio2018}. 

Undoubtedly, one of the most relevant direct-search algorithms is the \textit{\ac{NMS}} \cite{Nelder1964}. \textit{\ac{NMS}} is a local \ac{SOO} direct-search algorithm that exploits a simplex to guide the search towards a locally optimal solution. The \textit{\ac{NMS}} algorithm envelopes a region of the design space using a simplex, which is a generalization of a triangle to arbitrary dimensions, i.e., a triangle in two dimensions, a tetrahedron in three dimensions, etc. The simplex is then successively modified using operations, like reflection, expansion, contraction, and shrinking, that iteratively replace the simplex's worst vertex values. % http://www.scholarpedia.org/article/Nelder-Mead_algorithm
Unlike other direct-search algorithms, \textit{\ac{NMS}} requires no more than two function evaluations per iteration, except when applying the shrinking operation. The initial simplex highly influences the performance of the algorithm: while smaller initial simplices often lead to local searches that converge towards a local optimum, larger initial simplices allow the algorithm to cover a larger extent of the solution space, thus becoming more robust to local optima.

Another interesting simplex-based algorithm is \textit{SUBPLEX} \cite{Rowan1990}, which attempts to overcome the \textit{\ac{NMS}} difficulties when addressing higher dimensional problems by decomposing the problem in low\nobreakdash-\hspace{0pt}dimensional
subspaces. To that end, \textit{SUBPLEX} subdivides the design space in low-dimensional subspaces and then applies the \textit{\ac{NMS}} algorithm to the most promising subspaces, in order to seek for a better solution. %In contrast to \ac{NMS}, which has difficulties in high-dimensional problems, \textit{SUBPLEX} reduces the limitations through the decomposition of the problem in low-dimensional subspaces which are more efficiently optimized by \ac{NMS}.
In the same vein, \textit{\ac{PRAXIS}} \cite{Brent1973} also decomposes the problem in smaller ones, by considering each dimension of the solution space separately. Concretely, \textit{\ac{PRAXIS}} moves from one solution to another by iteratively finding better solutions in every other dimensions and then combining them into a candidate solution. 

Besides the local algorithms discussed so far, \textit{\ac{DIRECT}} is a very promising global algorithm. \textit{\ac{DIRECT}} \cite{Jones1993DIRECT} is a \ac{SOO} algorithm which recursively subdivides the design space into smaller multidimensional hyper-rectangles, each represented by a solution in their centre. For each solution, the objective function is evaluated, thus yielding an estimate of the quality of each rectangle. Based on these values, \textit{\ac{DIRECT}} focus the search on more promising regions of the design space, further subdividing those. To minimize the overall number of function evaluations, \cite{Gablonsky2001} suggested a modification to \textit{\ac{DIRECT}} to make it more efficient for functions with few local optima and a single global optimum. This modified algorithm, called \textit{\ac{DIRECT}-L}, differs from the original one by grouping the hyper-rectangles based on the size of the longest rectangle side and by allowing at most one subdivision in each group, i.e., at most one hyper-rectangle of each group can be subdivided in each iteration. These modifications to the original algorithm promote the reduction of the number of divisions, which has a direct impact on the overall number of function evaluations.

\textit{\ac{DMS}} \cite{Custodio2010} is a global multi-objective direct-search algorithmic framework which combines the main ideas of directional direct-search algorithms with the Pareto dominance concepts. In simple terms, \textit{\ac{DMS}} maintains a list of feasible nondominated solutions and their associated step sizes. Then, it iteratively selects nondominated solutions from this list and evaluates a few solutions along a predefined set of directions located at a distance determined by the solution's step size. If during the exploration better solutions are found, then the list is updated. On the other hand, if no better solutions are found, the associated step size is reduced. The process then repeats. This algorithmic framework extends to \ac{MOO} the directional type direct-search algorithms, such as pattern search \cite{Kolda2003}, among others. In addition to \textit{\ac{DMS}}, we refer \cite{Custodio2018} to the interested reader for a more recent direct-search \ac{MOOA}.

Overall, direct-search algorithms are not as popular as other classes of derivative-free algorithms. Nevertheless, their convergence proofs and the recent developments in the field of \ac{MOO} make this class very appealing for \ac{BPO}. 

\subsection{Metaheuristics Algorithms}
Metaheuristics are algorithms that employ simple mechanisms, called heuristics, to locate good solutions in complex design spaces, while considering the trade-off among precision, quality, and computational effort of the solutions. These algorithms often rely on randomization, and biological or physical analogies to perform robust searches and to escape local optima \cite{Glover2003Metaheuristics}. Additionally, through these heuristics, the designer is able to increase the overall performance by adding domain-specific knowledge. Moreover, their non-deterministic and inexact nature confer them the ability to effortlessly handle complex and irregular objective functions \cite{Wortmann2017GABESTCHOICE}.

Unfortunately, although metaheuristics can be good optimization algorithms when provided with sufficient amounts of time to do the necessary objective function evaluations \cite{Conn2009}, in some contexts, this is not possible. This is the case of \ac{BPO} in the architectural practice, where each evaluation is so time-consuming that the execution of thousands of evaluations rapidly becomes an infeasible scenario. Due to their stochastic nature, limiting the number of evaluations has severe repercussions on their convergence guarantees \cite{Hasancebi2009}.

Depending on the metaphors adopted by each algorithm, metaheuristics can be classified in different subclasses, including, among others, \acp{EA}, which explore Darwinian natural selection and evolution concepts (e.g., heredity, reproduction, genes, recombination, crossover, and mutation), Swarm algorithms, which explore collective intelligence through the cooperation of homogeneous agents in the environment (e.g., birds, ants, bees), and Physical Algorithms, which take inspiration from physical processes (e.g., the annealing process in metallurgy) \cite{Brownlee2011}.

% GA Explained
The \textit{\ac{GA}} is undoubtedly one of the most popular metaheuristics algorithms. As an \ac{EA}, it explores the mechanisms of biological evolution to search for better solutions. In particular, it randomly generates an initial set of individuals representing the candidate solutions, called population, which is then evolved for a specified number of iterations (or generations). The evolution process is comprised of four main phases: (1) adaptability, where individuals of the population are assigned a suitability or fitness value; (2) mating selection, where pairs of individuals are selected for reproduction, based on a probabilistic function which is usually proportional to each individual's fitness value; (3) crossover, where the genotypes of the selected individuals are recombined to produce new individuals; and (4) mutation, where new individuals are subjected to random copying errors with a certain probability. While in earlier generations, populations are usually more diverse, in final generations, their individuals are often very similar to the fittest individuals, i.e., we observe an intensification of the traits of the most suitable individuals, thus emulating the natural selection mechanism \cite{Brownlee2011}.

% NSGA-II
Along the same line, \textit{\ac{NSGA-II}} is a \textit{\ac{GA}} specially tailored for \ac{MOO} problems, which incorporates the ideas of population genetics and evolution, Pareto dominance, and a crowding measure to attain an approximation of the Pareto front that is both accurate and diverse \cite{Deb2002}. In simple terms, \textit{\ac{NSGA-II}} maintains an archive and a population, which are iteratively evolved until a stopping criteria is met. Every iteration, the algorithm combines the archive and the population and sorts their individuals according to their nondomination ranks, a measure of the number of Pareto fronts that have to be removed until an individual becomes nondominated. The individuals belonging to the same rank are also sorted according to a density measure, based on the average distance of their two closest individuals. Afterwards, the archive is replaced with the best $50\%$ individuals and the next offspring is produced based on the recombination and mutation of the individuals in the archive.  

% Physical Algorithm
In addition to \acp{EA}, other Physical Algorithms have also been successfully applied to address optimization problems. One example is the \textit{Simulated Annealing} algorithm. Resemblant of \textit{Stochastic Hill Climbing} algorithms, where new candidate solutions are randomly sampled and accepted if they represent an improvement, this algorithm iteratively re-samples the solution space aiming at finding an optimal solution. However, in contrast to those algorithms, the \textit{Simulated Annealing} algorithm may accept re-sampled solutions with lower performance, according to a probabilistic function that becomes more discerning of the quality of the samples as the number of iterations increases, thus resembling the natural annealing process \cite{Brownlee2011}.

% PSO
Although less explored, \ac{PSO} algorithms, such as \textit{OMOPSO} and \textit{SMPSO} \cite{Sierra2005OMOPSO,SMPSO}, have also been shown to yield promising results in some optimization problems, even surpassing some of the most reputed \acp{EA} \cite{Durillo2011SMPSO}. In their simpler versions, \ac{PSO} algorithms are global algorithms inspired by biological systems, such as the collective behavior of flocking birds or fish schooling, which interact and learn from one another to solve problems. In \ac{PSO}, the intelligence is decentralized, self-organized, and distributed throughout the participating particles, also known as swarm. These particles maintain information about their velocity, their current and individual best positions, and also the global best position known to the swarm. At each time step, the position and velocity of each particle are updated according to the best swarm or close neighbor position~\cite{Brownlee2011}.
	
Overall, metaheuristics are stochastic algorithms, mostly without convergence guarantees, usually requiring several hundreds or even thousands of evaluations in order to obtain good solutions. For problems involving expensive objective functions, like \ac{BPO}, these algorithms are usually not a good choice. Their overall time complexity becomes even more concerning in the case of \ac{MOO}, as the number of objectives increases. Unfortunately, there is a predominance of these algorithms (e.g., \textit{\ac{GA}}, \textit{\ac{SPEA2}}, and \textit{\ac{NSGA-II}}) among \ac{BPO} practitioners \cite{Wortmann2017GABESTCHOICE}, with very few tools providing support for other classes of algorithms. Nevertheless, they present several parameters that can be fine-tuned to improve their performance and, in fact, they can be very efficient solvers when properly configured. However, the optimal set of parameters is problem-specific and the same configuration applied to another problem might yield a bad performance.

\subsection{Model-based Algorithms}
Model-based algorithms are effective handlers for problems characterized by the large time complexity associated with the computation of the objective function and by the absence of information about the objective function \cite{Forrester2009SBO}. Model-based algorithms are able to provide faster estimates of a designâ€™s performance, by supplementing or replacing the original objective function with an approximation \cite{Wortmann2016BBO}. This approximation, called the surrogate, is generated from a set of known objective function values and is then explored to determine the promising candidate solutions to evaluate next. The results obtained from evaluating the candidate solutions are then used to improve the surrogate and this process is repeated until a stopping condition is satisfied \cite{Koziel2011}.

Despite having a well-defined analytical form, which makes computations on the surrogate model more efficient than on the original objective function, the surrogate is only an approximate representation of the original function, and, therefore, must be constantly updated to guarantee a reasonably accurate local representation~\cite{Koziel2011}.

Nowadays, the techniques applicable to the generation of surrogate models range from trust-region methods to \ac{ML} techniques. These techniques can be used to create (1) local surrogates, i.e., models where the approximation to the objective function is built around a certain point, and (2) global surrogates, i.e, models where the approximation is generated from all the obtained points. Whilst the former relies on the construction of simple, partial models of the objective function, the latter relies on the creation of a full model, which requires balancing the need for improving the model's accuracy by exploring broader regions in the solution space with the need for improving the objective function's value by exploiting promising regions~\cite{Koziel2011}. This balance is determined by a strategy that selects the next promising solutions to evaluate. 

Undoubtedly, the best feature of model-based algorithms is the reduction in total optimization time. This is particularly relevant in the context of \ac{BPO}, which involves time-consuming simulations. However, the lower availability and the lack of necessary technical knowledge to implement or incorporate these algorithms into optimization processes are still obstacles to a broader adoption of this approach. Notwithstanding the existence of different studies involving \ac{ML} techniques for the creation of full surrogate models~\cite{Koziel2011, Forrester2009SBO}, such as \ac{MLP}, \ac{SVR}, \textit{\ac{RBF}}, and \textit{\ac{RF}}, among others, only a few have actually been applied in the context of architecture. This scenario is even more self-evident when we shift from the single- to the multi-objective optimization context.

Two relevant local, trust-region, model-based algorithms are \textit{\ac{COBYLA}} and \ac{BOBYQA} \cite{Powell1994COBYLA, Powell2009BOBYQA}. The former uses the concept of a simplex to iteratively generate linear approximations of the objective function, whereas the latter generates quadratic approximations instead.

One relevant global model-based algorithm uses \textit{\acp{RBF}} to create a global approximation of the objective function, as represented in \cref{eq:rbf} \cite{Forrester2009SBO}. This approximation is the weighted sum of $N$ radial basis functions $\phi_i: \mathbb{R}^+ \to \mathbb{R}$, each associated with a different center $x_i$, and weighted by an appropriate coefficient $w_i$. These weights are then estimated based on the interpolation of data and are iteratively updated to successively improve the approximation. Loosely speaking, radial basis functions are (1) radial functions whose value depends on the distance of a point $x$ to a given center $x_i$, and (2) basis functions, i.e., functions that can be linearly combined to produce the set of continuous functions in the function space. Examples of commonly used radial basis functions are the linear ($\phi(r) = r$), the cubic ($\phi(r) = r^3$), and the thin plate spline ($\phi(r) = r^2 \ln r$), among others.

\begin{equation}\label{eq:rbf}
\hat{f}(x) = \sum_{i=1}^{N}w_i\phi_i(\left\lVert x-x_i \right\rVert)
\end{equation}

Overall, model-based algorithms are techniques that create approximations of the original expensive objective functions and that usually explore other algorithms, like \textit{\acp{GA}}, \acp{MOEA}, or even sampling algorithms, to seek for better solutions. Particularly, global model-based algorithms exhibit very good performance at initial stages of an optimization process, especially when compared with metaheuristics.