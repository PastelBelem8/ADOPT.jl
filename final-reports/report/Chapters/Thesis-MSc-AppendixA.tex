% #############################################################################
% This is Appendix A
% !TEX root = ../main.tex
% #############################################################################
\chapter{Algorithms' Definitions}
\label{appendix:AlgorithmsDefinitions}

A very summarized explanation will be provided for the optimization algorithms studied during this dissertation. Therefore, we suggest \cite{BlumRoli2003Metaheuristics,Glover2003Metaheuristics,Zhou2011} for more complete explanations on metaheuristic algorithms, \cite{Koziel2011} for model-based algorithms, and \cite{Conn2009,Custodio2010,Custodio2018} for direct-search algorithms.

\section{Sampling Algorithms} 
\begin{itemize}
\item \textbf{Full-factorial} generates a complete set of design solutions. Each dimension is described by a discrete set of values, called \textit{levels}. All possible combinations of these levels are generated and used to represent the design space and, thus, minimize errors when running full-factorial techniques multiple times. The number of generated samples scales exponentially with the number of dimensions. 
	
\item \textbf{Monte Carlo Sampling} (or Random Sampling)~\cite{Giunta2003DOE}, originally known as pseudo-Monte Carlo sampling or pseudo-pandom sampling, generates designs randomly. It frequently yields large unexplored regions of the design space. To circunvent this limitation, users should generate several hundreds or thousands of solutions.

\item \textbf{Stratified Monte Carlo Sampling}~\cite{Giunta2003DOE} yields a more uniform sampling of the design space. Each dimension of the design space is subdivided into bins of equal probability and, then, a sample is randomly selected from each bin. Provides better coverage of the design space and allows the user to select the number of bins to create in each dimension. However, the number of generated samples scales exponentially with the number of dimensions.

\item \textbf{Latin Hypercube Sampling} (LHS)~\cite{Giunta2003DOE} provides a better distribution of the generated samples by subdividing each dimension in equally sized bins. The generated samples are randomly placed in different bins, so that when observing the one-dimensional projections of the samples and bins, there is only one sample in each bin. The number of samples scales linearly with the number of dimensions.
\end{itemize}

\section{Direct-Search Algorithms}
\begin{itemize}
	\item \textbf{Nelder-Mead Simplex} (NMS)~\cite{Nelder1964} is a local direct-search algorithm that exploits a simplex to enforce convergence towards an optimal solution. The NMS algorithm envelopes a region of the design space using a geometric figure, called simplex, which is a generalization of a triangle or tetrahedron to arbitrary dimensions (e.g., a triangle in two dimensions or a tetrahedron in three dimensions). The simplex is then successively modified using operations, like reflection, expansion, contraction, and shrinking, that iteratively replace the simplex's worst vertex values. Unlike other direct-search algorithms, NMS requires no more than two function evaluations per iteration, except when applying the shrinking operation. The initial simplex has a high impact on the algorithm's performance, particularly, smaller initial simplices frequently lead to local searches and, consequently, to local optimum.
	
	\item \textbf{Subspace simplex} (Subplex)~\cite{Rowan1990} algorithm is an unconstrained local optimization algorithm~\cite{Rowan1990}. As a generalization of the NMS algorithm, SUBPLEX subdivides the design space in low-dimensional subspaces and then applies the NMS algorithm to the most promising subspaces, in order to seek for a better solution. In contrast to NMS, which has difficulties in high-dimensional problems, SUBPLEX reduces the limitations through the decomposition of the problem in low-dimensional subspaces which are more efficiently optimized by NMS.
	
	\item \textbf{Principal Axis} (PRAXIS)~\cite{Brent1973} moves from one region to other by iteratively applying line search algorithms to each direction. The goal of these line search algorithms is to determine a better solution along a certain dimension. It is a local algorithm, which strongly depends on a good initial point.
	
	\item \textbf{DIRECT}~\cite{Jones1993DIRECT} is an algorithm that recursively subdivides the design space into smaller multi-dimensional hyper-rectangles, estimating the quality value of each rectangle. DIRECT uses these values to focus the search on more promising regions of the design space and to further subdivide those in smaller hyper-rectangles. DIRECT is designed to fully explore the variable space, even after finding one or more local optima.
	
	\item \textbf{DIRECT-L} \cite{Gablonsky2001} is a a local variant of DIRECT claimed to be more efficient for functions with few local optima and a single global optimum. The main differences to the original DIRECT algorithm are that DIRECT-L groups the hyper-rectangles based on the size of the longest rectangle side and that there is at most one subdivision in each group, i.e., at most one hyper-rectangle of each group can be subdivided in each iteration. These modifications to the original algorithm promote the reduction of the number of divisions, which has a direct impact on the overall number of function evaluations.
	
	\item \textbf{Direct MultiSearch} DMS \cite{Custodio2010} is a global multi-objective direct-search algorithmic framework which combines the main ideas of directional direct-search algorithms with the Pareto dominance concepts. It maintains a list of feasible nondominated solutions and corresponding step size parameters, which is iteratively updated during the optimization process. From this list, it explores the vicinities of a selected nondominated solution using the associated step size and a predefined set of directions. During the local search around the selected solution, it creates a temporary list of points that contains all the solutions of the first list and the solutions evaluated during this search. The temporary list is then filtered removing any unfeasible or dominated solution. If the temporary list differs from the first list, then it means that better solutions were found and, consequently, the first list is replaced by the temporary one and the process repeats.  On the other hand, if the temporary list equals the first list, it means that the local search was not able to find a better solution and the step size associated to this solution should be reduced. The process then repeats. This algorithmic framework extends to \ac{MOO} the directional direct-search algorithms, including the generalized pattern search and mesh generating set search~\cite{Kolda2003}, among others. 
	 %https://www.mat.uc.pt/~lnv/papers/dms.pdf
	% http://ferrari.dmat.fct.unl.pt/personal/alcustodio/multiglodspaper.pdf


\end{itemize}	

\section{Metaheuristics Algorithms}
\begin{itemize}
	
	\item \textbf{Simulated Annealing} \cite{Brownlee2011} is a global optimization physical algorithm. Resemblant of hill climbing algorithms, where new candidate solutions are randomly sampled, this algorithm iteratively re-samples the solution space aiming at finding an optimal solution. During the search, the algorithm is propitious to accept the re-sampled solutions with lower performance, according to a probabilistic function that becomes more discerning of the quality of the samples during the search, thus resembling the natural annealing process. 
	
	\item \textbf{Controlled Random Search 2} (CRS2) algorithm is a variant of the CRS algorithm for global optimization~\cite{Price1983}. A CRS algorithm is a population-based random search algorithm that creates an initial set of points, the population, which are then randomly evolved by means of heuristic rules. In the original CRS algorithms, heuristic rules modify a point at a time, replacing the worst point with a better one (called trial point), using a technique resemblant of the NMS algorithm \cite{Nelder1964}. Similarly to NMS, CRS algorithms use a simplex, i.e., a generalized triangle in $N$ dimensions, to envelope a region described by a random subset of points in the population. The worst point of the population is replaced with the reflection of the worst point in the simplex~\cite{Kaelo2006CRS2}. The CRS2 variant differs from the original in that it assumes that the worst population point will always be a part of the simplex. One variant of the CRS2 algorithm introduces a local mutation component in an attempt to overcome situations,  where heuristic rules constantly fail to find a trial points that actually improve the worst point. Fundamentally, this local mutation generates a second trial point that results from the exploitation of the region around the best point in the population~\cite{Kaelo2006CRS2}.  
	
	\item \textbf{Evolutionary Strategy with Cauchy Distribution} (ESCH) \cite{Santos2010} explores evolutionary strategies to seek more efficient solutions. Initially, it creates a population that is iteratively recombined during the optimization according to a non-uniform distribution function and then mutated using a combination of two different mutation operators, such as the Gaussian variation and the gene duplication.
	
	\item \textbf{Improved Stochastic Racking Evolutionary Strategy} (ISRES) \cite{Runarsson2000} is a global evolution strategy method that evolves candidate solutions by stochastically ranking and selecting the best solutions, i.e., a method that maximizes the suitability of the candidate designs given by a fitness ranking function. The candidate designs are evolved iteratively by combining the application of a mutation rule and differential variation.
			
	\item \textbf{Covariance Matrix Adaptation - Evolutionary Strategy} (CMA-ES) \cite{Hansen2006} is a descendent of the evolutionary strategies, whose candidate solutions are sampled according to normal distributions and recombination and mutation amount to selecting new mean values for the distribution and to adding perturbations with zero mean, respectively. CMA-ES maintains a covariance matrix with the pairwise dependencies between the variables in the distribution which is updated using a technique called covariance matrix adaptation (CMA) which attempts to incrementally update the likelihood of previously successful candidate solutions.
	
	\item \textbf{SPEA2} \cite{Zitzler2001SPEA2} is a \ac{MOEA} that incorporates evolutionary mechanisms to incrementally evolve a population from generation to generation. In addition to the population, SPEA2 maintains a set of optimal solutions called archive. While a stopping criteria is not met, the archive is continuously updated by ranking each individual of the population according to the solutions it dominates. Any dominated solution will be removed from the archive and, in case the archive exceeds its size, a clustering technique selects individuals in more crowded regions of the objective space for removal. Binary tournaments are then carried between elements in the archive and population to select the individuals for reproduction and to create the individuals for the next generation, i.e., the offspring. These individuals result from the recombination and mutation of the best individuals from the previous iteration. 
	
	\item \textbf{NSGA-II} \cite{Deb2002} is a \ac{MOEA} that also incorporates evolutionary mechanisms to incrementally evolve a population. In contrast to SPEA2, NSGA-II does not maintain an archive, instead evaluating a combination of parent and children populations. Initially, a parent population is created and sorted according to its nondomination rank. The nondomination rank is determined as follows: for each individual of the population, determine the nondominated ones and assign those individuals a rank of 1, then, temporarily remove those individuals and, for the remaining individuals, determine the nondominated ones, assigning those individuals a rank of 2, repeating this process repeat until every individual is assigned a rank. After assigning these ranks, NSGA-II applies binary tournaments, based on the nondomination rank, to select the individuals to recombine and mutate in order to create the initial children population. Afterwards, both parent and children populations are combined. For each individual, NSGA-II computes the nondomination rank and a density of estimate based on the average distance of the two closest individuals. The best individuals compose, what the algorithm calls a new parent population, which is then sorted according to their rank and density estimates. Again, the parent population is subject to binary tournaments, which now consider both the nondomination rank and the density measure, to select the individuals for reproduction. These individuals are then recombined and mutated, thus producing the new children population. The process then repeats until a stopping criteria is met, combining the new parent and children populations and recomputing their nondomination rank and diversity measures and generating the new populations for the next iteration.
	
	\item \textbf{\ac{MOEA} with Decomposition} (MOEA/D) \cite{Zhang2007MOEAD} is a \ac{MOEA} based on the decomposition (e.g., weighting sum approach) of an optimization problem into smaller subproblems. All subproblems are solved simultaneously by evolving a population of solutions, which mantains the best solutions found for each problem. Each subproblem is assigned a vector defining the aggregation coefficients, i.e., the coefficients defining the weights of each objective for each subproblem. As a result, the relation between different subproblems is established based on the distance of their coefficient vectors, thus implying that neighboring subproblems yield similar solutions. At each iteration, MOEAD/D maintains (1) a population of the current solutions for each subproblem, (2) an archive with the nondominated solutions that are found during the search, and (3) a list of the best values found for each objective. Initially, the algorithm computes the neighbors of each subproblem by comparing the Euclidean distances between the coefficient vectors of each subproblem, generates the initial population randomly and uses a problem-specific method to initialize the list of the best values for each objective. Then, for each iteration, it iterates every subproblem and, for each subproblem, randomly selects two neighboring subproblems. Using the current solutions found for each of those subproblems, use genetic operators to create a new solution which is then, optionally, improved using a problem-specific heuristic. If the resulting solution yields a better objective value than the corresponding best objective value, stored in the list of best values, replace that objective's value by the one obtained with the new solution. After determining the solution for one subproblem, update the information in other subproblems, and, if no archive solution dominates the newly created solution, add it to the archive and remove any solution in the archive that is dominated by it. This process is repeated until a stopping criteria is satisfied.
	
	\item \textbf{Epsilon-\ac{MOEA}} ($\epsilon$-MOEA) \cite{Deb2003EpsMOEA} exploits the $\epsilon$-dominance concept and more efficient archive and population update strategies. $\epsilon$-dominance promotes a better diversity of solutions, by not considering two nondominated solutions with a difference smaller than $\epsilon_i$ in the $i$-th objective to be optimal. Similarly to SPEA2, $\epsilon$-MOEA maintains two set of solutions - an archive and a population. In each iteration, $\epsilon$-MOEA creates multiple offspring solutions. Each offspring is created through the recombination and mutation of two solutions which were chosen from the archive and the population. Although the archive's solution is selected either randomly or based on a relation with the solution from the population (e.g., proximity), the population's solution results from the random selection of two individuals in the population and a subsequent nondomination check, which aims at selecting the final one. After generating the offspring, each solution is evaluated and compared to the archive and population for their inclusion. Regarding the inclusion in the archive, if an offspring member is $\epsilon$-nondominated regarding all the members of the archive, then it is incorporated into the archive. On the other hand, each offspring is compared against the whole population and, if the offspring reveals to be nondominated, it replaces one of the dominated members. Otherwise, the offspring replaces a randomly chosen member in the population. This process is repeated until a stopping criteria is met and, in that case, the final archive is returned as the optimal solutions.
	
	\item \textbf{Pareto Envelope-Based Selection Algorithm 2} (PESA2 or PESA-II) is similar to the PESA algorithm, but instead of exploiting individual-based strategies to select individuals for reproduction, it uses region-based ones. Indeed, PESA is a \ac{MOEA} that maintains an archive with the nondominated solutions from which the algorithm then selects the individuals to reproduce using a tournament selection operator. To this end, PESA subdivides the objective space into hyperboxes and assigns a fitness value to each archive solution. The fitness value of each solution corresponds to the number of solutions that are enclosed within the same hyperbox as that solution. However, this selection scheme is based on each individual's fitness value and does not necessarily ensure a good distribution of the solutions across de solution space.
	
	\item \textbf{Pareto Archived Evolution Strategy} (PAES) \cite{Knowles1999} is a local \ac{MOEA} which explores \acp{ES} on populations composed of a single individual and maintains an archive with previously found solutions to identify the approximate dominance ranking of the current and candidate solution vectors. In every iteration, the candidate solution is compared with respect to the current solution and depending on whether the candidate dominates the current solution or not, the current solution will be updated or a new candidate solution is found through mutation. In the case where none of the solutions dominate each other, each solution is compared against an archive of the nondominated solutions and if the candidate solution dominates any of the solutions in the archive, it will be accepted as the new current solution and the dominated solutions are removed from the archive, otherwise, the selection of the new current solution will be based on how crowded each solution is, when compared to the solutions in the archive. 
	
	\todo{Trabalhar mais nesta descrição...}
	\item \textbf{Hypervolume Estimation Algorithm for \ac{MOO}} (HypE)~\cite{Zitzler2011HypE} is an algorithm that explores the \ac{HV} indicator to rank individuals. Particularly, to minimize time penalties associated with the computation of the \ac{HV}, in scenarios with more than three objectives, HypE uses Monte Carlo simulations to estimate the \ac{HV} value of each individual.
	% 	Unfortunately, algorithms incorporating the Pareto dominance relation and diversity measures appear to have difficulties in optimization scenarios with more than two objectives, which spurred the development of algorithms using other quality measures, including quality indicators. To overcome the objective limitation and provide the user with the flexibility to use a more efficient algorithm, Octopus provides the option to use HypE, an algorithm that explores the \ac{HV} indicator to rank individuals. Particularly, to minimize time penalties associated with the computation of the \ac{HV}, in scenarios with more than three objectives, HypE uses Monte Carlo simulations to estimate the \ac{HV} value of each individual~\cite{Zitzler2011HypE}. 
	
	\item \textbf{Speed-constrained Multi-objective \ac{PSO}} (SMPSO)
	
	\item \textbf{OMOPSO}
	
\end{itemize}

\section{Model-based Algorithms}
\begin{itemize}
	\item \textbf{GPR-based} algorithms rely on guassian process (GP) techniques, which generalize Gaussian probability distributions to functions. GP algorithms incorporate a measure of the similarity between points, called the kernel function, to predict the value for an unseen point from training data. Besides the prediction of the value, GPs provide a measure of the associated uncertainty.
	
	\item \textbf{MLP-based} algorithms consist on the creation of surrogate models using \acp{MLP}. \ac{MLP} is a class of feedforward neural networks that comprises at least three layers of nodes, including one input layer, one output layer, and, at least, one hidden layer. In neural networks, all nodes with the exception of the input nodes are called neurons and each node possesses a non-linear activation function. These non-linearities provide the networks the ability to approximate non-linear and extremely complex functions. These \acp{MLP} models are trained using a supervised learning technique, where the model is provided with the input variables' values and the outcome that was observed for those input values. The model is then updated using a technique called backpropagation which uses the error rates between the obtained outcome and the expected outcome to correct the neurons and improve the \ac{MLP} model. 
	
	\item \textbf{RBF-based} algorithms consist on the creation of surrogate models using \acp{RBF}. Since its invention, multiple implementations of \ac{RBF} have been proposed. RBFOpt implements two well-known versions, namely, the Gutmann's~\cite{Gutmann2001} and the Regis and Shoemaker's~\cite{Regis2007}, commonly known as MSRSM. These techniques differ in the search strategy for the next candidate solution to be evaluated using the original objective function. The former uses the solution, which is likely to yield the largest improvement in the surrogate's accuracy, whereas the latter tries to balance the surrogate's accuracy amelioration with the exploitation of promising solutions, using other search strategies, such as genetic algorithms, sampling algorithms, or other mathematical solvers~\cite{Wortmann2017Opossum}. Moreover, RBFOpt provides five different types of radial basis function: linear, multi-quadratic, cubic, thin plate spline, and automatic selection, which are also provided in the Opossum interface.
		
	\item \textbf{RF-based} algorithms consist on the creation of surrogate models using \acp{RF}, also called random decision forests~\cite{Ho1995RDF}. These algorithms incorporate an ensemble learning approach that consists on applying multiple decision trees to obtain a better performance. Given a set of input variables the result of an \ac{RF} algorithm will be given by the mean  of the results returned by each individual tree. \ac{RF} are particularly sought due to their ability to correct the decision trees propensity to overfit the data, i.e., to become too specific.
	
\end{itemize}