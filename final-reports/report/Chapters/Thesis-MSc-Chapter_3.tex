% #############################################################################
% This is Chapter 3
% !TEX root = ../main.tex
% #############################################################################
% Change the Name of the Chapter i the following line
\fancychapter{Algorithmic Framework for Architecture}
\cleardoublepage
% The following line allows to ref this chapter
\label{chap:architecture}

This dissertation focus on the study of optimization algorithms that are able to handle more efficiently a set of problems involving time-consuming functions. Apart from the theoretical study and literature review, this dissertation proposes a framework to enable the application of different types of optimization algorithms, both for\ac{SOO} and \ac{MOO} problems. The concretization of such framework requires: (1) the definition of an abstraction layer, that enables the modeling of optimization problems, (2) a wide variety of optimization algorithms to address different optimization problems, (3) a set of performance indicators to provide the user with a measure of the algorithms' quality when testing multiple algorithms, and (4) visual representations of the obtained results to provide more comprehensive feedback over the optimization results.   

With the aforementioned framework, users are able to address optimization problems involving time-intensive functions, for which the objective functions' analytical forms are unavailable. In order to verify its executability and suitability for conducting multiple optimization processes, we developed a prototype. This chapter describes the main components of the developed prototype and also the main requisites that led to their implementation. 

\section{Programming Language}

One of the first concerns when developing the prototype was regarding which programming language to use. The currently implemented prototype uses two programming languages, which were chosen based on the available optimization libraries, the domain of the solution, and the performance of each programming language.
 
On the one hand, several languages already provide well-known optimization libraries, which have been tested, and independently validated. As a result, instead of attempting to implement the optimization algorithms again, we sought for programming languages which provided an easy access to these implementations. On the other hand, because we are dealing with optimization algorithms that involve numerical computations, we aimed at languages which provided fast numeric computation without the need to dive into lower level languages, such as C or Fortran. Finally, the performance of the programming language and also the mechanisms provided by the language to make code development more efficient (e.g., avoiding boilerplate code).

The first programming language is Julia\footnote{https://julialang.org}, a recent programming language that has been shown to be very promising in the numerical optimization domain, and that provides mechanisms to speed code development (e.g., functional programming paradigm, macros). Despite having a smaller library's repository than, for instance, Python\footnote{https://www.python.org/}, Julia has grown to implement some of Python's optimization libraries (e.g., Scikit-learn, NLopt), and, therefore, also provides some of the same functionalities of Python.   

Unfortunately, Julia is a juvenile language and, whilst some of these libraries can effectively become more efficient (e.g., Scikit-learn), other libraries present some limitations regarding the supported functionalities. For instance, in terms of data processing and visualization, Python presents a more stable  and better documented set of libraries (e.g., Pandas, Seaborn, Matplotlib, Plotly). For all these reasons, in order to facilitate the processing and visualization of the optimization results, we adopted Python as the second programming language.

% Furthermore, the data transfer between Julia and Python is easily achieved by using the mechanisms provided by Julia (e.g., macros) and a wrapper library, PyCall. \todo{Nao sei muito bem como melhorar isto.. Mas o vocabulario est√° um bocado simplorio...}

One aspect to take into consideration is that, albeit being implemented in Julia and Python, our solution could have been implemented in other programming languages such as Java or C++, which also provide several optimization and data processing libraries. In this dissertation and as it will be explained in ~\Cref{chap:implement} and \Cref{chap:evaluation}, we had the extra motivation that our case studies concerning Building Design Optimization (BPO) were already developed in Julia. Thus, by creating an abstraction layer in Julia, we were able to easily integrate optimization processes with an existing algorithmic tool in architecture, and to easily test the effectiveness and quality of the proposed solution within the architectural context. 

\section{Abstraction Layer}

The abstraction layer is designed to abstract the user from the logic of other external optimization tools. By providing a uniform \ac{API}, i.e., a set of primitives and procedures, this abstraction layer eliminates the appearent chasm between different optimization tools and facilitates the use of algorithms from different optimization libraries, incurring no additional efforts for the user, i.e., the user is not required to make any further modifications to the program, e.g., changing the program structure, changing the primitive operations, in order to test different optimization algorithms. 

By adopting one of the currently available domain-specific modeling languages for optimization (e.g., JuMP, PyOMO, MultiJuMP, PISA, AMPL), not only would users have access to a well-tested and more stable \acp{API}, but they would also have out-of-the-box access to a wide variety of optimization solvers capable of handling various problems. 

Notwithstanding their benefits, the abstraction layer provided in this dissertation does not make use of these modeling languages due to (1) their lack of support for both \ac{SOO} and \ac{MOO} problems, (2) complexity of syntax rules and need for deep understanding of the language to use it, and (3) lack of solvers or mechanisms that enable the execution of optimization solvers capable of efficiently addressing optimization problems involving costly evaluation functions for which information about the objective functions is unavailable. As a result, to explore one of these modeling languages would require not only additional developments to create the mechanisms to support both single- and multi-objective problems, but also to create a new solver capable of addressing the specific type of problems that this dissertation set out to address. 

Nevertheles, influenced by existing optimization modeling languages, the set of operations that compose the abstraction layer removes the complexity associated to most modeling languges and exposes a set of simple primitives that can be combined to yield successively incrementaly complex optimization problems. Moreover, the abstraction layer was designed to enable the execution of any of the optimization approaches discussed in~\Cref{sec:optimizationproblems}, ranging from \ac{SOO} to the different \ac{MOO} approaches and passing through the design of experiments approach. 

As described in \Cref{chap:back}, the definition of an optimization process is composed of two parts: the modeling of the optimization problem and the optimization algorithm. This \ac{API} provides not only simple and intuitive primitives to define variables, the objective functions, and, when necessary, the constraints, but also empowers users with the ability to use and fine-tune the parameters of each optimization algorithm, i.e., parameters that control the detail of the search process. The entire \ac{API} is intuitive and simple to use by less experienced users, but flexible to be extended and improved by programmers, which may decide to add more algorithms or integrate other optimization libraries.

\section{Optimization Algorithms}

Optimization algorithms represents a crucial part in an optimization process and, consequently, in the developed solution. In order to study different optimization algorithms and to test their suitability for specific optimization problems~\cite{Wolpert1997NFLT}, a large set of distinct and representative optimization algorithms should be provided. In addition to diversity, our solution also provides the users with the flexibility to configure the different parameters associated to the optimization algorithms, commonly known as hyperparameters. By fine-tuning the values of these hyperparameters for a specific problem, the performance of optimization optimization can be drastically improved. 

One other important requisite considered by the developed solution is that at least a subset of the provided algorithms must be tailored to address optimization problems that incorporate time-consuming evaluation functions and for which information is difficult to obtain. To address this requisite, we reviewed the most reputed \ac{SOO} and \ac{MOO} open-source derivative-free optimization libraries. After a thorough analysis of different optimization libraries, the final solution exposed a total of 36 algorithms from 4 different libraries, listed in \Cref{table:algorithms}. 

To implement sampling algorithms, the prototype makes use of Julia's arrays and random primitives, requiring no additional libraries to be implemented. Secondly, by integrating the NLopt~\cite{NLOPT} library within our prototype, we were able to expose 10 \ac{SOO} algorithms and, thus, to test algorithms from the three derivative-free optimization classes. Moreover, this library has already been used in Goat~\cite{GOAT}, an architectural design optimization tool, which has been applied in some architectural studies as well\todo{REF - Wortmann}. Thirdly, the Platypus library was chosen from a set of \ac{MOO} libraries, that included Platypus, jMetal, MOEAFramework, Paradiseo, and PaGMO. The Platypus library was the final choice due to the variety and quality of the \ac{MOO} algorithms provided, the ease of integration with the Julia programming language, and the cleanness and readibility of its \ac{API}. Finally, the fourth library was the Python scikit-learn library, which provides the basis algorithms for constructing more complex model-based algorithms, both for single- and multi-objective problems.

\begin{table}[]
	\centering
	\begin{tabular}{cccll}
		\rowcolor[HTML]{EFEFEF} 
		\textbf{Class} & \textbf{Objectives} & \textbf{Global} & \textbf{Algorithm} & \textbf{Library} \\ \hline
		& Both & G & K-factorial & None \\
		& Both & G & Latin Hypercube & None \\
		& Both & G & Random & None \\
		\multirow{-4}{*}{Sampling} & Both & G & Stratified Random & None \\ \hline
		& S & G & CRS2 & NLopt \\
		& M & G & CMA-ES & Platypus \\
		& S & G & ESCH & NLopt \\
		& M & G & ES & Platypus \\
		& M & G & $\epsilon$-MOEA & Platypus \\
		& S & G & GA & Platypus \\
		& M & G & GDE3 & Platypus \\
		& M & G & IBEA & Platypus \\
		& S & G & ISRES & NLopt \\
		& M & G & MOEAD/D & Platypus \\
		& M & G & NSGA-II & Platypus \\
		& M & G & NSGA-III & Platypus \\
		& M & G & PAES & Platypus \\
		& M & G & PESA2 & Platypus \\
		& M & G & SPEA2 & Platypus \\
		& M & G & OMOPSO & Platypus \\
		\multirow{-17}{*}{Metaheuristic} & M & G & SMPSO & Platypus \\ \hline
		& S & G & DIRECT & NLopt \\
		& S & G & DIRECT-L & NLopt \\
		& S & L & NMS & NLopt \\
		& S & L & PRAXIS & NLopt \\
		\multirow{-5}{*}{Direct search} & S & L & Subplex & NLopt \\ \hline
		& S & L & BOBYQA & NLopt \\
		& S & L & COBYLA & NLopt \\
		& Both & G & Decision Tree & Scikit-learn \\
		& Both & G & GP & Scikit-learn \\
		& Both & G & Linear & Scikit-learn \\
		& Both & G & MLP & Scikit-learn \\
		& Both & G & Random Forest & Scikit-learn \\
		& Both & G & RBF-Cubic & pySOT \\
		& Both & G & RBF-Linear & pySOT \\
		\multirow{-10}{*}{Model-based} & S & G & SVR & Scikit-learn
	\end{tabular}
	\caption[List of the algorithms exposed by the solution's prototype]{List of the algorithms exposed by the prototype, discriminated by class, number of objectives, aim of the search, and the optimization library implementing it. L - Local, G - Global, M - Multi, S - Single.}
	\label{table:algorithms}
\end{table}

\section{Performance Indicators}

One of the goals of this thesis is to compare different optimization algorithms and assess the influence of their inherent mechanisms and assumptions in the overall performance of optimization processes. To this end, it is necessary to have a simple way of verifying the quality of these algorithms. 

To this end, the solution enriches the abstraction layer previously discussed with a set of performance indicators. While for comparing \ac{SOO} algorithms, these indicators usually resume to discovering both the value of the best found solution and the number of evaluations (or time) required to reach it, for comparing \ac{MOO} algorithms this solution implements a subset of the indicators discussed in \Cref{ssec:performance}, namely, the unary indicators: \ac{ONVG}, \ac{ONVGR}, spacing, spread, maximum spread, \ac{ER}, \ac{GD}, \ac{HV}, and \ac{IGD}, and the binary indicators: R-metrics, $\epsilon$ indicators, and the two set coverage.

Notwithstanding the utility of quantitatively qualifying the performance of different algorithms, this information can be difficult to interpret by less experienced users. To enhance interpretability, the proposed solution introduces post-processing and visualization functionalities discussed in the next section. Particularly, in the architectural context, where architects often lack knowledge about optimization, visual and post-processing mechanisms become very important to promote a better comprehension and enable the making of more informed decisions. 

\section{Visualization and Post-Processing Mechanisms}



- Logs and Traceability
- No interactive but easily extensible
- Explores Python's libraries (Pandas, Numpy, Matplotlib, Seaborn, Plotly)


Traceability and state feedback

The solution also provides integrated visualization mechanisms that aim to complement and enrich the information extracted during an optimization run. 

Our solution also values the traceability of results especially for enhancing user comprehension. To improve existing mechanisms, our solution produces files involving all the necessary information about the configurations (e.g., algorithm parameters) and the solutions evaluated during the optimization process. Using these files, we are able not only to input them to other post-processing tools (e.g., visualization, statistics), but also to hot start and pause/resume optimization processes.




\section{Summary}
In order to verify the executability of the proposed framework, we developed a simple prototype using the Python and Julia\footnote{https://julialang.org/} programming languages. Python was selected, because it already provides countless optimization and visualization libraries, thus fostering the rapid development of 


At the light of the architectural practice, our solution makes use of the textual programming paradigm and, consequently, has a special affinity with textual \ac{AD} tools (e.g., Khepri). As a result, when coupled with these \ac{AD} tools, our solution also benefits from their portability and scalability properties. We aim at reducing the abnormal time-complexity of \ac{BPO} by providing model-based algorithms. 

Finally, we consider the complexity of our solution. Unlike the analyzed tools, our solution does not benefit from the visual paradigm, which means that it should be simple to use and intuitive, even for non-programmers. As a result, we hide the complexity of the integration of optimization libraries under an abstraction layer, providing a clean and succinct set of primitives. These primitives draw inspiration from simple optimization mathematical models and should be rather intuitive and easy to use. 

