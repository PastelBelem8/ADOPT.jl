% #############################################################################
% This is Chapter 4
% !TEX root = ../main.tex
% #############################################################################
% Change the Name of the Chapter i the following line
\fancychapter{Evaluation}
%\cleardoublepage
\label{chap:evaluation}

The main goal of this dissertation was to identify single- and multi-objective optimization algorithms capable of handling the computationally complex problems that characterize \ac{BPO}, and to devise strategies for its efficient application to architectural design optimization problems. To attain this goal, we proposed and implemented an optimization framework which enabled not only the application of different optimization approaches, but also the use of several algorithms within each approach.  Finally, we claimed that the proposed framework allows architects to easily explore automated optimization processes within architectural practices. 

This chapter focuses on the evaluation of the proposed framework from both a qualitative perspective and a quantitative one. In particular, \cref{sec:qualitative} discusses the qualitative properties of the optimization framework, especially, for \ac{BPO} practices, outlining its advantages and disadvantages. \Cref{sec:quantitative} tests different optimization algorithms, measuring their adequacy for different \ac{BPO} problems. To this end, we follow the algorithmic framework described in \cref{chap:implement}, which exploits the optimization framework described in \cref{chap:architecture} and the Khepri \ac{AD} tool. 

The evaluation aims to answer the following questions: 
\begin{itemize}
	%\item Do the studied algorithms present benefits for the architectural practice? 
	\item Is there a single algorithm that can consistently, across all case studies, reach the best solution?
	\item Is there any class or subclass of algorithms that constantly outperforms others?
	\item Can any of the algorithms reduce the impact of the expensive simulations typically performed in building design? 
	\item Is the proposed framework adequate for performing optimization in architectural practices? 
	\item How does the proposed framework benefit architectural practices?
\end{itemize}
In the following sections, we will address and answer these questions.

\section{Qualitative Evaluation}
\label{sec:qualitative}

The qualitative evaluation of optimization frameworks involves considering multiple aspects, including the flexibility, adaptability, diversity of algorithms, and ease of use, among others. Calling upon the No Free Luch Theorem (\ac{NFLT}) discussed in \cref{ssec:comparisondfo}, some algorithms can be good solvers for some problems but poor solvers for others~\cite{Wolpert1997NFLT}. As a result, selecting the right algorithm can have a great impact in the efficiency of optimization processes. Particularly in building design, to benefit from such performance gains, the diversity of algorithms allows facing each problems' characteristics differently, enabling the identification of the most promising algorithms. Additionally, to be easily used by less experienced users, algorithms should be effortlessly run, without the need for many manual changes. Notwithstanding their innate simplicity, optimization frameworks should also be flexible enough to enable more experienced users to fine-tune them, thus fostering more efficient optimization processes. At last, a good framework should be able to adapt to handle different problems.

Regarding the adaptability of the proposed framework, it provides mechanisms to address both single- and multi-objective problems: $15$ \ac{SOO} algorithms and $13$ \acp{MOEA}, respectively. Simpler approaches, like the experimental one, discussed in \cref{ssec:doe}, are also possible using one of the $5$ sampling methods available in the framework. Moreover, a core feature of this framework is that it provides $10$ \ac{ML} algorithms to be combined with other algorithms, thus promoting the potential reduction of the time complexity involved in building design. 

Back in \cref{chap:architecture}, we discriminated all the algorithms available in the framework (see \cref{table:algorithms}). When comparing to existing optimization tools in architecture, our solution presents a more extensive and diverse set of algorithms, which can be explored to address a wider variety of problems. Moreover, while existing tools rely on a single optimization approach, our solution supports different ones.

Unlike competing optimization tools, which are implemented on top of visual programming languages, like Grasshopper and Dynamo, our framework is currently implemented on top of Khepri, a textual-based \ac{AD} tool. Although the visual paradigms' graphical feel provides a more comfortable experience to less experienced users, the fact that our framework makes use of Khepri confers it more scalability and portability. %, thus allowing users to seamlessly apply optimization to more complex buildings. 
Moreover, to make it more appealing to less experienced users, the developed optimization framework is setup as a ready-to-use tool, where every parameter of the optimization process is configured by default. Nevertheless, the framework also supports detailed configurations of the different algorithms, thus allowing more experienced users to fine-tune and, if desired, to combine different algorithms. 

Given the importance of testing different algorithms before settling for a single one \cite{Wortmann2016BBO}, the developed framework provides the necessary mechanisms to facilitate the automated testing of multiple algorithms. Conversely, competing tools frequently require users' intervention to test different algorithms, either by dragging other optimizer components and making necessary changes in the design program or, when possible, by simply re-configuring the optimization tool to use one of the other supported algorithms. 

Regarding post-processing and logging mechanisms, the proposed framework is automatically configured to produce complete log files, including all the information about the algorithms and problems being addressed, as well as a real-time log of the different solutions explored during the optimization run. This differs from existing architectural optimization tools, which only produce the log files after the execution. Nevertheless, some of the currently existing optimization tools still visually outperform the developed framework. In fact, while the framework's current implementation provides functionality to read the log files and produce the corresponding plots, some of the existing architectural optimization tools update the information of the optimization process in real-time. Note, however, that users are also able to visualize a more updated view of the optimization process as long as they re-run the visualization functionality. 

Finally, in order to use the optimization framework, users only need to create their design's parametric description and to model the corresponding optimization problem, which involves the definition of the variables and their bounds, of the objectives, and, if necessary, of the constraints. In contrast to the existing tools, which require the modification of the optimization program, our framework requires no such efforts to use different algorithms and/or optimization approaches. Instead, users are only required to specify the algorithms that they wish to apply and to configure them accordingly.

\begin{table}[htbp]		
	\centering
	\caption[Comparison between the proposed solution and the analysed optimization plug-ins]{Comparison between the proposed solution and the analysed optimization plug-ins. S - single, M - multi, G - Global, L - Local, Meta - Metaheuristics, Model - Model-based.}
	\label{table:pluginsquantitative}
	\includegraphics[width=\textwidth]{Images/Evaluation/qualitativecomparison.pdf}
\end{table}

In sum, \cref{table:pluginsquantitative} shows a comparison between the most competing architectural optimization plug-ins and the solution proposed in this dissertation. Note that, unlike other optimization plug-ins, our solution does not provide a well-defined \ac{GUI}. Therefore, for the proposed solution, the analysis depicted in the table takes into account the user's experience when using the solution's \ac{API}.

% #############################################################################
\section{Quantitative Evaluation}
\label{sec:quantitative}

In this section, we evaluate the suitability of the proposed framework to tackle optimization problems characterized by computationally complex objective functions. Given that the framework makes several optimization algorithms available, the effectiveness of the framework is highly dependent on the effectiveness of these algorithms and, thus, we will evaluate them on a series of computationally complex optimization problems within the architectural practice. These results can then be explored by the users of the framework to select the best optimization algorithms to use for a given problem.

%In order to identify and explore the effectiveness of algorithms within building design practices, we evaluated the performance of several algorithms using the proposed optimization framework. Moreover, we evaluated the real applicability of such framework in three case studies, two of which were proposed by an architectural studio.%: (1) the optimization of the lighting conditions of a solarium in a private house in Portugal; (2) the optimization of the structural behavior and elegance of an arc-shaped space frame structure; and (3) the optimization of the cost and the lighting conditions of an exhibition room in an urban museum.   

When measuring the optimization algorithms' effectiveness, several factors must be considered: (1) the optimization time is sensitive to the computational power of the machine where the algorithm is being run, (2) the non-determinism of several algorithms, and (3) the algorithms' hyperparameters. Firstly, to remove the time dependency of the machine characteristics and objective function's complexity, we measure the performance of the optimization algorithm in terms of the number of function evaluations, which is proportional to the actual time spent by the optimization process. Secondly, the stochastic nature of several optimization algorithms (e.g., random initial points, random modifications to solutions) might yield different results even when ran twice under the same configurations. To address this limitation, we run each stochastic algorithm three times and we use the average of the values to draw conclusions. Finally, for each problem, the algorithm's performance can be better or worse depending on its configurations. To this end, we opt for using the default algorithm's configurations, thus emulating the case when the architect's knowledge does not suffice to properly fine-tune the algorithm. 

%\todo{ESCLARECER diferença entre combined PFs, true PFs, Approximated PFs}
% #############################################################################
\subsection{Single-Objective Optimimization: Ericeira House}
\label{ssec:soocasestudy}
The first case study involved the optimization of the daylight conditions of a room in an isolated private house in Portugal~\cite{Caetano2018,Belem2018optimizeddesign}. The room was designed with a set of façade shading panels that modulate the daylight conditions on the interior of the room. The panels are composed of a set of horizontal wood bars of different sizes, which alternate between one full-length bar and a set of smaller bars. For aesthetics reasons, the size and position of the smaller bars along the panel's width were randomized. The final pattern of the façade's shading panels was defined in terms of the length’s step, the maximum distance separating two consecutive bars, and the minimum and maximum lengths of the smaller bars. Initially, the goal was to find a solution for the shading panels that maximized the room's daylight performance, which was measured using the \ac{sUDI} metric~\cite{Nabil2006}. While, in general, it is known that the more openings in the shading panels, the more daylight enter the room, this may also cause situations of uncomfortable glare. As a result, these situations should be accounted for, and, notwithstanding the values of the optimization process, architects should always perceive optimization results with a critical thinking. \Cref{fig:ericeira_multiple_panels} represents some design variations, ranging from denser patterns, with lower \ac{sUDI} values, to sparser ones, with higher \ac{sUDI} values.

%\begin{figure}[htbp]
%	\centering
%	\includegraphics[width=\textwidth]{Images/Evaluation/Ericeira_1.jpg}
%	\caption{Ericeira Solarium: Representation of the shading panels' geometric pattern and design variables.}
%	\label{fig:ericeira_panels_explanation}
%\end{figure}

\begin{figure}[htpb]
	\centering
	\includegraphics[width=\textwidth]{Images/Evaluation/Ericeira_2.png}
	\caption[Ericeira Solarium: Different representations of the shading panels’ geometric pattern]{Ericeira Solarium: Representation of the shading panels’ geometric pattern with different sUDI values (from left to right, 7\%, 62\%, 90\%, and 100\%).}
	\label{fig:ericeira_multiple_panels}
\end{figure}

Initially, we pursued a simple experimental approach to address this optimization problem \cite{Caetano2018}. To generate the design variants to evaluate, we used the \textit{Monte Carlo} and the \textit{Latin Hypercube sampling} algorithms. This approach allowed us to exploit previous knowledge about the shading panels and its impact on the daylight conditions of the room (e.g., that sparser patterns increase daylight performance) and, consequently, to produce a more efficient optimization process. In fact, based on this knowledge, we set several iterations of the different sampling methods and, within each iteration, we have further restrained the variables' to vary in smaller ranges, thus enforcing the sampling of incrementally more efficient designs. %\Cref{fig:ericeira_doe} shows an example of the results obtained during this process, as well as the solutions that were presented to the architects, so that they would choose the one that better suited their intentions.

%\begin{figure}[htbp]
%	\centering
%	\includegraphics[width=\textwidth]{Images/Evaluation/Ericeira_caadria2018.PNG}
%	\caption{Ericeira Solarium: The scatter plot with the samples obtained during the design of experiments approach. The models a. to g. correspond to the set of examples presented to the architects.}
%	\label{fig:ericeira_doe}
%\end{figure}

Despite achieving optimal solutions with values of sUDI of $100\%$, we have only achieved these solutions after $200$ function evaluations. Because each evaluation took approximately $7$ minutes to complete on a dual \textit{Intel Xeon CPU E5-2670 @ 2.60GHz with 64GB RAM}, the optimal solution was only obtained after $1400$ minutes, or, equivalently, $23.33$ hours. This large time complexity resulted from the fact that approaches based on sampling algorithms consist on the consecutive uniformed experimentation of multiple designs, i.e., without taking into consideration previous design evaluations, often evaluating irrelevant design solutions. Moreover, this approach required several manual interventions (e.g., analyse the results, redefine the variables' bounds, select number of evaluations). In an attempt to minimize the time complexity of the optimization process and to verify the impact of more guided approaches in \ac{BPO} problems, we have also tackled this problem using a \ac{SOO} approach \cite{Belem2018optimizeddesign}. Particularly, we evaluated the performance of $13$ different derivative-free optimization algorithms: $5$ direct-search, $3$ metaheuristics, and $5$ model-based. Given the time complexity of each function evaluation, we set a limit of $60$ function evaluations per run.

%http://papers.cumincad.org/data/works/att/caadria2018\_278.pdf
\begin{table}[htbp]
	\centering
	\caption[Ericeira Solarium: Mean best results and evaluations discriminated per algorithm]{Ericeira Solarium: Table with the mean best daylight results and mean evaluations to reach optimal solutions of each algorithm. Results are averaged over $3$ runs, each with $60$ evaluations.}
	\label{table:phase1results}
	\includegraphics[width=\textwidth]{tables_and_code/Ericeira_phase1_stats_v1.pdf}
\end{table}

\Cref{table:phase1results} shows the mean best results and the standard deviation of the three runs, discriminated by algorithm. According to the results, in average, the global model-based algorithms \textit{\ac{GPR}}, \textit{\ac{RBF}CC}, and \textit{\ac{RBF}CL} were able to find an optimal solution within the first $30$ evaluations. Conversely, the local model-based algorithms \textit{\ac{COBYLA}} and \textit{\ac{BOBYQA}} performed rather poorly in this problem, converging to far from optimal solutions after $29$ and $48$ function evaluations, respectively. Regarding direct-search algorithms, the global algorithm \textit{\ac{DIRECT}} was able to find a close to optimal solution (with an \ac{sUDI} value of $98\%$) in the last function evaluation. Its local variant, \textit{\ac{DIRECT}-L}, and the local direct-search algorithms \textit{\ac{PRAXIS}} and \textit{SUBPLEX} fell short of the expected and barely managed to improve over $80\%$. Nevertheless, the simplex-based direct-search algorithm \textit{\ac{NMS}} performed surprisingly well, having achieved an average result of $89.67\%$ within the first $15$ evaluations. Finally, although metaheuristics performed better than most local model-based and direct-search algorithms, they seem to stagnate in design solutions with \ac{sUDI} values below the $88\%$, after $30$ evaluations.

\Cref{fig:phase1results} shows the average performance per algorithm class, also separating them in local or global algorithms. Overall, local algorithms seem to perform worse than all other algorithms, with local direct-search and model-based algorithms stagnating towards design solutions with \ac{sUDI} values below $75\%$ and $70\%$, respectively. Contrastingly, global algorithms were able to find design solutions with values of \ac{sUDI} larger than $80\%$. Despite the good initial performance of metaheuristics algorithms for the first $20$ evaluations, global direct-search algorithms quickly surpassed them, achieving close to optimal solutions with \ac{sUDI} values of $90\%$. Lastly, global model-based algorithms were, on average, the best performing algorithms, achieving close to optimal solutions shortly after $24$ evaluations. 

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\textwidth]{Images/Evaluation/Ericeira_results_ph1_per_class.PNG}
	\caption[Ericeira Solarium: Mean best daylight results in function of the number of evaluations, discriminated per class of algorithms]{Ericeira Solarium: Mean best daylight results in function of the number of evaluations, discriminated per class of algorithms.}
	\label{fig:phase1results}
\end{figure}

Given the overall bad performance of local algorithms, we decided to assess their performance when initialized with different solutions. Notwithstanding their ability to quickly converge to locally optimal solutions, the quality of the found solutions highly depends on the solution used to initialize the search. Therefore, we have also studied the impact of different initial solutions in the performance of these algorithms. To this end, we tested all $5$ local algorithms with two different initial solutions: a bad solution, with a $7\%$ value of \ac{sUDI}, and a reasonable solution with a $78\%$ value of \ac{sUDI}. Moreover, we decided to further restrict the number of evaluations to $15$, thus emulating an hypothetical scenario, where users lack knowledge about different optimization algorithms and opt for testing several of them. Ideally, this would allow them to infer the most promising algorithm and obtain a reasonable solution to hot-start other algorithms and, potentially, improve the overall optimization time.

\Cref{fig:phase2results} presents the mean best daylight results found by each local optimization algorithm. As expected, no local algorithm was able to obtain a good solution when provided with a bad starting solution. On the one hand, when provided with a mild initial design, both \textit{\ac{COBYLA}} and \textit{\ac{NMS}} found the best designs achieving a \ac{sUDI} value of $99\%$. On the contrary, \textit{\ac{PRAXIS}} found the worse, and showed no relevant improvement over the initial design. Nevertheless, it initially managed to outperform other methods, achieving values of \ac{sUDI} of $80\%$. After $8$ evaluations, \textit{\ac{COBYLA}} and \textit{\ac{NMS}} quickly converged to near optimal designs, with \ac{sUDI} values of $99\%$ and $98\%$, respectively. \textit{\ac{BOBYQA}} and \textit{SUBPLEX} struggled to improve from the initial design.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{Images/Evaluation/Ericeira_results_ph2.PNG}
	\caption[Ericeira Solarium: Mean best results of daylight performance in function of the number of evaluations, discriminated per local algorithm]{Ericeira Solarium: Mean best results daylight results as a function of the number of function evaluations, discriminated per local algorithm. Algorithms suffixed with \textit{HS} are given an initial solution with an \ac{sUDI} value of $78\%$, whilst algorithms suffixed with \textit{BS} are given an initial solution with an \ac{sUDI} value of $7\%$.}
	\label{fig:phase2results}
\end{figure}

On the other hand, when provided with a bad initial design, the best daylight result has an \ac{sUDI} value of $15\%$ and was found by \textit{\ac{NMS}} after $9$ evaluations. \textit{\ac{NMS}}, \textit{\ac{COBYLA}}, and \textit{SUBPLEX} exhibit similar performance, stagnating in a design with an \ac{sUDI} value of $11\%$ after $3$ evaluations, with \textit{\ac{NMS}} being able to further improve the design after $5$ evaluations. \textit{\ac{PRAXIS}} exhibits the worst performance among all methods, showing no significant improvements throughout the whole optimization process.


% #############################################################################
\subsection{Multi-objective Optimization}

In this section, we evaluate two \ac{MOO} problems \cite{Belem2019MOO,IP2019MOO}. As previously discussed in \cref{ssec:performance}, addressing these problems comprises a difficult task, not only because of the higher complexity of the obtained results, but also because of the absence of standards regarding the best way to evaluate the \acp{MOOA}' performance. For these reasons, in this dissertation, we opted for evaluating the algorithms' performance in terms of the results returned by each algorithm, i.e., the \acp{aPF}. For each algorithm, we compute different indicators that measure the \acp{aPF} in terms of their cardinality, diversity, and accuracy.%: \ac{ONVGR}, \ac{ER}, Spacing, Maximum Spread, \ac{MPFE}, \ac{GD}, and \ac{HV}. 

Although some indicators measure aspects based exclusively on the \acp{aPF}, others require a reference set to compare with the \acp{aPF}. Ideally, this reference set would represent the real optimal solutions for the specified problem. Unfortunately, this set of optimal solutions, also called \ac{tPF}, is not known for most \ac{BPO} problems. In an attempt to better approximate it, we compute a fictitious Pareto Front, the \ac{cPF}, composed of the best solutions found by each algorithm. 

Note, however, that we aimed at measuring the average performance of each algorithm regarding an unknown \ac{tPF}. In general, computing an accurate approximation of the \ac{tPF} would require running the algorithms for thousands of iterations, which is not feasible in most \ac{BPO} problems involving time-consuming evaluation functions. As a consequence, we adopted a methodology similar to the one used in \cref{ssec:soocasestudy}, which quantifies the performance of each algorithm in terms of the mean value of three runs, using as reference, for each run, the corresponding \acp{cPF}. 

In the next sections, we present and discuss the obtained results for each algorithm. In \cref{appendix:appendixB}, we provide additional information about each algorithm's configuration and present the Pareto front plots corresponding to each run.

\subsubsection{Space Frame Optimization}
Motivated by the interest of architects in performing structural analysis \cite{Cichocka2017SURVEY}, the first \ac{MOO} case study consisted in the optimization of both the structural behavior and an \textit{ad-hoc} measure of the irregularity of an arc-shaped space frame. To instil irregularities in the space frame, we introduced three attractors that cause a deformation in the shape of the truss, each of which is defined in terms of its fixed-radius cylindrical coordinates in the arc-shaped space frame~\cite{Belem2019MOO}. To measure the goals for each design variant, we used (1) the Robot analysis tool to compute the maximum displacement of the structure, and (2) the sum of the Euclidean distances between the attractors. To increase the interest of this case study, we set out to minimize both objectives, thus promoting the conflict between them: placing the attractors near each other will weaken the structure and, thus, increase the maximum displacement of the space frame. In fact, to reduce the maximum displacement, the attractors should be scattered across the space frame but this implies larger distances among the three attractors, thus worsening the second objective. \Cref{fig:spaceframe} illustrates three examples of the space frame structure. 
\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\textwidth]{Images/Evaluation/truss-kat-small.png}
	\caption[Space Frame: Representation of three space frame design variants]{Space Frame: Representation of three design variations of the arc-shaped space frame, with copper balls representing the three attractors.}
	\label{fig:spaceframe}
\end{figure}

To optimize the space frame, we decided to test $10$ metaheuristics and $9$ model-based algorithms. On the one hand, each metaheuristic algorithm comprised a total of $15$ individuals/particles per iteration, which were evolved for $15$ iterations. On the other hand, model-based algorithms derived $100$ initial samples using the Latin Hypercube sampling algorithm, which were then used to create the initial approximation to the expensive evaluation function, upon which another $125$ evaluations were completed. Overall, every algorithm was limited to a total of $225$ function evaluations, each taking approximately $40$ seconds to complete on a dual \textit{Intel Xeon CPU E5-2670 @ 2.60GHz with 64GB RAM}. In total, each run is composed of $4275$ candidate solutions and takes approximately $2$ days to complete.

\Cref{table:spaceframe,table:spaceframestd} show the mean results of the performance indicators and corresponding standard deviations for the three runs, discriminated by the algorithms' classes and subclasses. As previously mentioned, we computed several performance indicators for each \ac{aPF}, which provide information about: (1) cardinality, measured by \ac{ONVGR} and \ac{ER}; (2) diversity, measured with Spacing and Maximum Spread; and (3) accuracy/convergence, measured with \ac{MPFE} and \ac{GD}. Moreover, we used a Pareto-compliant indicator, the \ac{HV}, to obtain a combined measure of all the three mentioned aspects. % To simplify the performance comparison among the different algorithms, we restrained the set of indicators to the unary ones. 
% 1 - Two set coverage não ia dar medidas relevantes porque raramente as diferentes frentes se tocam. 
% 2 - Epsilon indicators poderia ser interessante, mas não foi testada a implementação
% 3 - R-metrics requer funções de utilidade que não temos e que requer alguma sensibilidade em relação ao problema...

% The overall cardinality of each combined Pareto front is 14, 24, and 19, respectively.
When considering the cardinality aspect, the \ac{ONVGR} column of \cref{table:spaceframe} presents the ratio of optimal solutions between \acp{aPF} and \acp{cPF}. In general, metaheuristics seem to retrieve the most nondominated solutions within each run, whereas model-based algorithms seem to retrieve the least. In fact, among the model-based algorithms, the algorithms exploring random search strategies, i.e., the algorithms suffixed with \textit{Random}, yield fewer nondominated solutions, which may result from a poor exploration of the solution space. On average, the best performing algorithm, \textit{PAES}, is able to find twice the number of solutions that compose each \ac{cPF}, whereas model-based algorithms, including \textit{GPR+Random} and \acp{MLP} algorithms, struggled to find a set of optimal solutions with at least half of the size of the \acp{cPF}. 
%http://papers.cumincad.org/data/works/att/caadria2018\_278.pdf
\begin{table}[h!]
	\centering
	\caption[Space Frame: Mean values for the performance indicators results, discriminated per algorithms]{Space Frame: Mean values for the performance indicators results, discriminated by algorithm. Results are averaged over $3$ runs, each with $225$ evaluations.}
	\label{table:spaceframe}
	\includegraphics[width=\textwidth]{Images/Evaluation/caadria/Results_Mean_20190428.PNG}
\end{table}

While the \ac{ONVGR} indicator provides an intuition about the richness of each algorithm's \acp{aPF}, many of the identified solutions might not be truly optimal, i.e., despite being optimal among all the evaluated solutions, these solutions might not belong to the corresponding \ac{cPF}. To this end, \ac{ER} is used to measure the percentage of false-optimal solutions for each algorithm. In this case, it becomes clear that even though \textit{PAES} retrieves twice as many optimal solutions as the \acp{cPF}, most of them are not truly optimal. Moreover, none of the solutions found by the $\epsilon$\textit{-MOEA}, \textit{MOEA/D}, and \textit{CMA-ES} metaheuristics algorithms belong to the \acp{cPF}. The same happens with some of the model-based algorithms, particularly, \textit{GPR+NSGA-II}, \textit{GPR+Random}, and \textit{MLP+Random}. On the other hand, \ac{PSO}-based metaheuristics algorithms, \textit{SMPSO} and \textit{OMOPSO}, exhibit the lowest \ac{ER} value, having found at least one optimal solution in each run.

The diversity aspect consists in the analysis of the distribution of the nondominated solutions across the objective space. The Spacing indicator measures the uniformity of the \acp{aPF}, regardless of the \acp{cPF}. Considering this indicator, the two \ac{ES} metaheuristic algorithms, \textit{PAES} and \textit{CMA-ES}, and one \ac{EA}-based metaheuristic algorithm, $\epsilon$\textit{-MOEA}, achieved the most uniform \acp{aPF}. Conversely, model-based algorithms seem to yield more irregular \acp{aPF}, namely, \textit{MLP+NSGA-II} and \textit{MLP+SMPSO} achieved the worst values of the Spacing indicator. Note, however, that this indicator merely provides an idea of the regularity of distribution of the solutions. Ideally, this indicator would also suggest a good coverage of the \ac{cPF}, i.e., that the \acp{aPF} found by each algorithm cover the same regions as the \acp{cPF}, instead of focusing on narrower regions. However, most of the algorithms that present the best Spacing scores achieve such values because most of the identified optimal solutions lie within the same small region but present a more uniform distribution. Besides this limitation, these indicators are also highly sensitive to outliers and to the number of retrieved solutions. % number of solutions also indirectly contributes to these scores of each indicators, since these indicators measure the distances between consecutive optimal solutions and the weight each outlier has in smaller or larger sets can greatly influence the final result.

Besides having an uniform distribution, when applicable, good Pareto fronts should also cover large extents of the objective space in order to provide more relevant trade-offs. \textit{Max Spread} measures the extent of the \acp{aPF} retrieved by each algorithm. On average, \textit{SMPSO}, \textit{GDE3}, and \textit{OMOPSO} are able to explore wider extents of the objective space. Observing \cref{table:spaceframe}, we conclude that, on average, \textit{SMPSO}, \textit{GDE3}, and \textit{OMOPSO} were able to cover the objective space better. Contrastingly, the \ac{ES}-based algorithms were the worst algorithms in this aspect, having explored smaller regions of the objective space. Regarding model-based algorithms, it is possible to observe that the ones based on \textit{SMPSO} were able to explore broader regions of the objective space than the ones based on \textit{NSGA-II} or \textit{Random} strategies. 
\begin{table}[]
	\centering
	\caption[Space Frame: Standard deviation values for the performance indicators results, discriminated by each algorithm]{Space Frame: Standard deviation values for the performance indicators results, discriminated by algorithm. Results are averaged over $3$ runs, each with $225$ evaluations.}
	\label{table:spaceframestd}
	\includegraphics[width=\textwidth]{Images/Evaluation/caadria/Results_Std_20190428.PNG}
\end{table}

Another important aspect of \acp{aPF} is their accuracy and how close their solutions are from the \acp{cPF}. In this case study, we considered two accuracy indicators, \ac{MPFE} and \ac{GD}. On average, when considering \ac{MPFE}, every model-based algorithm retrieved \acp{aPF} whose \ac{MPFE} values were always better than the ones obtained by any metaheuristic. In particular, \textit{MLP+NSGA-II} and \textit{MLP+Random} have the smallest maximum error which means that all the points are at most at that distance from an optimal solution. Furthermore, the algorithms exploring a larger extent of the objective space, i.e., with higher values of Max Spread, have worse \ac{MPFE} values. This can be explained due to the lack of information about the \ac{tPF}, which is only being approximated by the best solutions found in each run, i.e., the \acp{cPF}. 

Notwithstanding the fact that \ac{MPFE} provides an estimate of the maximum error of the algorithms' \acp{aPF}, this indicator does not provide a real measure of how close the results are to the \acp{cPF}. To this end, we use the \ac{GD} indicator, which measures the average approximation of the \acp{aPF} retrieved by each algorithm to the closest solutions in the corresponding \acp{cPF}. \Cref{table:spaceframe} shows that, on average, \textit{MLP+NSGA-II} and \textit{PAES} present the best convergence towards the \ac{cPF}, and that \textit{GDE3} and \textit{OMOPSO} present the worst convergence values. These can be explained by the number of the nondominated solutions retrieved by each algorithm, as well as by the creation of clusters of optimal solutions near the \acp{cPF} that were discovered by \textit{PAES} and \textit{MOEA/D}, as is visible in \cref{sec:spaceframeoptimizationextra}. In general, other model-based algorithms also present reasonable scores, like the \textit{MLP+Random} or all the \ac{RF}-based algorithms even surpassing many metaheuristics algorithms, including \textit{CMA-ES}, $\epsilon$\textit{-MOEA}, \textit{NSGA-II}, and \textit{SPEA2}, thus suggesting better approximations.

In the end, we also used the \ac{HV} indicator, as it appraises the quality of a Pareto front with regards to all three aspects simultaneously. The best performing algorithms were the \ac{PSO}-based algorithms, \textit{SMPSO} and \textit{OMOPSO}, followed by \textit{GDE3}. Surprisingly, the \ac{PSO} model-based algorithms also present a good performance, when compared to other metaheuristics, and even to other model-based algorithms that explore \textit{Random} or \ac{EA} strategies to search for optimal solutions. Conversely, the worst performing algorithms were the \ac{ES}-based ones, \textit{CMA-ES} and \textit{PAES}, followed by \textit{GPR+Random}. % Finally, comparing different algorithms regarding \ac{IGD}, the \ac{PSO}-based metaheuristic algorithms still yielded the best results, whilst \ac{ES}-based metaheuristic algorithms yielded the worst. However, \textit{MOEA/D} unexpectedly reveals itself as the third best performing algorithm when considering the \ac{IGD} indicator. Although this seems odd, this value can be explained by the difference in the scales of both axis and to the higher density of Pareto optimal solutions in the $x$-axis for values between $1.2$ and $1.3$.
\begin{figure}[hptb]
	\centering
	\includegraphics[width=\textwidth]{Images/Evaluation/caadria/All_Algorithms_all_runs-2019-04-13_1000dpi.png}
	\caption[Space Frame: Pareto front plot]{Space Frame: Algorithms' \acp{aPF} measuring the attractors distance, in function of the maximum displacement. These fronts are obtained by combining the values of the $3$ runs for each algorithm. The \ac{cPF} is formed by finding the nondominated solutions from all the evaluated solutions.}
	\label{fig:allruns}
\end{figure}

Overall, no single algorithm was able to outperform the others in terms of all the indicators. Nevertheless, the \ac{PSO}-based metaheuristics algorithms, \textit{OMOPSO} and \textit{SMPSO}, exhibited the overall best performance. Moreover, even though none of the model-based algorithms was able to surpass the \textit{SMPSO} and \textit{OMOPSO}, the model-based algorithms that use \textit{SMPSO} also exhibited a reasonable performance, better than several well-known metaheuristics, including $\epsilon$\textit{-MOEA}, \textit{MOEA/D}, \textit{CMA-ES}, and \textit{SPEA2}. \Cref{fig:allruns} presents a combined view of all the algorithms for every run, where it is possible to visualize the extent of \ac{PSO}-based algorithms and the high density region to which several \acp{EA} and \acp{ES} algorithms converged.

\subsubsection{Black Pavilion: Skylights Optimization}
This case study addresses a real design problem where a two floor service building, called Black Pavilion, will be adapted to work as a museum~\cite{Caetano2018,IP2019MOO}. The architects planned to incorporate a skylight in the second floor of the building to improve the daylight conditions of an art exhibition space. The problem was to find the height, the width, the length, and the material of the skylight that maximized the daylight conditions (measured using the \ac{sUDI} metric \cite{Nabil2006}) while minimizing the cost of the skylight (approximated using a formula that depends on the skylight dimensions). In \cref{fig:blackpavilion}, we present an example of the exhibition space and the daylight incidence that results from placing the skylight in the ceiling.
%\begin{equation} \label{eq:costanalysis}
%cost\_function(width, length, height) = (width * length * 185) + ((width + length) * ( 2 * height) * 80)
%\end{equation}

Architects' previous knowledge did not suffice to determine the optimal configurations for the skylight. In general, it is known that the creation of higher skylights incur more costs and worse daylight conditions, as, in this case, higher skylights imply more direct light towards smaller areas in the exhibition space. Therefore, the architects fixed the height to be $1.5$\metre. Also, taking into account the fabrication costs, the team constrained the width and length to vary in multiples of $0.1$\metre. As a result, instead of modeling continuous variables, these were modeled as discrete variables.
\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{Images/Evaluation/BlackPavilion/PavPretoExample116x7204.png}
	\caption[Black Pavilion: Representation of the arts exhibition space with a skylight]{Black Pavilion: Representation of the arts exhibition space with the skylight in the top right corner.}
	\label{fig:blackpavilion}
\end{figure}

To respond to the architects' request, we conceived an algorithmic description of the building using the Khepri \ac{AD} tool and we used the Radiance lighting analysis tool to measure the \ac{sUDI} of different designs. Regarding the optimization, we decided to measure the performance of $10$ \acp{MOOA} regarding several indicators. Similarly to the previous case study, we aimed at providing an average prediction of the behavior of each algorithm in this specific problem. As a result, we derived the \acp{cPF} for each run and used them to compare the different algorithms. Each run comprises $200$ function evaluations, each of which takes on average $13.33$ minutes on a dual \textit{Intel Xeon CPU E5-2670 @ 2.60GHz with 64GB RAM}. In total, each run is composed of $2000$ candidate solutions and takes approximately $19$ days to complete.

Among the available algorithms and due to time constraints, we selected $4$ metaheuristics due to their acknowledged performance in other engineering applications: two \ac{EA}, \textit{SPEA2} and \textit{NSGA-II}, and two \ac{PSO}, \textit{SMPSO} and \textit{OMOPSO}. Additionally, we tested $6$ model-based algorithms, including \textit{RF+Random}, \textit{RF+NSGA-II}, \textit{RF+SPEA2}, \textit{GPR+Random}, \textit{GPR+NSGA-II}, and \textit{GPR+SPEA2}. \Cref{table:blackpavilion} shows the mean results of each algorithm for each of the evaluated performance indicators. 

\begin{table}[]
	\centering
	\caption[Black Pavilion: Mean values for the performance indicators results, discriminated by algorithm]{Black Pavilion: Mean values for the performance indicators results, discriminated by algorithm. Results are averaged over $3$ runs, each with $200$ evaluations.}
	\label{table:blackpavilion}
	\includegraphics[width=\textwidth]{Images/Evaluation/BlackPavilion/Results_Mean_20190428.PNG}
\end{table}

Firstly, regarding the results' cardinality, all algorithms exhibit an \ac{ONVGR} value inferior to $1$, which means that no algorithm was able to retrieve as many optimal solutions as the \acp{cPF}. Nevertheless, \textit{GPR+Random} and \textit{RF+NSGA-II} achieved the best results, attaining, on average, more solutions than all other algorithms. Conversely, \textit{GPR+NSGA-II} and \textit{GPR+SPEA2} obtained fewer solutions and were, therefore, the worst algorithms. Even though \textit{GPR+Random} retrieved, on average, the most solutions, note that this value is subject to large variations, as evidenced in \cref{table:blackpavilionstd}, and, in fact, when examining the results per run, \textit{GPR+Random} retrieved $13$, $14$, and $24$ solutions. Regarding metaheuristic algorithms, \acp{EA} have achieved relatively higher cardinalities than \acp{PSO}. 

Notwithstanding the cardinality of the \acp{aPF}, it is important to determine how many of these solutions actually lie in the \acp{cPF}, which is measured by the \ac{ER} indicator. In this case, \textit{RF+Random} presents an \ac{ER} value smaller than $0.5$, which means that, on average, more than half of the retrieved solutions lie in the \acp{cPF}. The second and third best performing algorithms were \textit{RF+NSGA-II} and \textit{NSGA-II} with values of $0.61$ and $0.64$, respectively. Conversely, \textit{SPEA2} and \textit{GPR+Random} presented the worst \ac{ER} values. Interestingly, even though \textit{GPR+Random} returned on average more solutions than all other algorithms, it barely succeeds in retrieving one optimal solution lying in the \ac{cPF}.

Secondly, when considering the mean dispersion of the \acp{aPF} found by each algorithm in the objective space, \textit{RF+Random} and \textit{NSGA-II} exhibit the first and second best results in terms of the Spacing indicator. On the other hand, \textit{GPR+NSGA-II} presents the worst value, hence suggesting that the solutions retrieved by this algorithm are not evenly spread across the objective space. While \textit{OMOPSO} presents the second worse Spacing value, this result is associated to a large deviation, which derives from the fact that, in the third run, the algorithm was able to retrieve a more uniform distribution (see \cref{sec:blackpavilionextra}). Interestingly, \textit{RF+NSGA-II}, presenting the third worse Spacing value, also presents a large standard deviation. Upon a careful review over this algorithm's \acp{aPF}, it is possible to observe this variation is explained due to a single solution in the second run that is offly distant from all others, which highly influences the results. 

When considering the maximum extent of the algorithms' \acp{aPF}, \textit{GPR+Random} and \textit{GPR+NSGA-II} are the most performing algorithms, achieving values of $0.64$ and $0.75$, respectively. Conversely, the other \ac{GPR}-based algorithm, \textit{GPR+SPEA2}, achieved the second worse performance with a value of $0.57$, only surpassing the performance of \textit{NSGA-II} and \textit{RF+SPEA2}, whose value was $0.56$. 

Thirdly, we measured the mean accuracy of the \acp{aPF} retrieved by each algorithm. In terms of \ac{MPFE}, \textit{GPR+SPEA2} deviates the least from the \acp{cPF}, with a mean value of $306.85$ for the furthest solution. The second best algorithm is also a model-based algorithm, \textit{RF+Random}, whose furthest found solution lies, on average, within $351.74$ from the \acp{cPF}. The third and fourth best performing algorithms were \ac{PSO}-based algorithms, which achieved close to optimal solutions as well. Conversely, \textit{GPR+NSGA-II} and \textit{GPR+Random} achieved the worst results in terms of \ac{MPFE}, identifying nondominated solutions at a distance of $1425.57$ and $3016.98$ from the closest solution in the corresponding \acp{cPF}. A noteworthy observation is that while the worse performance of \textit{GPR+Random} results mainly from its poorer performance during the second run (see \cref{sec:blackpavilionextra}), the same does not happen with \textit{GPR+NSGA-II}, whose \ac{MPFE} results for the first and third run remain approximately on $1600$. 

\begin{table}[htbp]
	\centering
	\caption[Black Pavilion: Standard deviation values for the performance indicators results, discriminated by algorithm]{Black Pavilion: Standard deviation values for the performance indicators results, discriminated by algorithm. Results are averaged over $3$ runs, each with $200$ evaluations.}
	\label{table:blackpavilionstd}
	\includegraphics[width=\textwidth]{Images/Evaluation/BlackPavilion/Results_Std_20190428.PNG}
\end{table}

In addition to \ac{MPFE}, we measured the \ac{GD} of each \ac{aPF}. Although the results for this metric resemble the ones obtained with \ac{MPFE}, there are a few differences, namely, in terms of the best performing algorithm. In this case, \textit{RF+Random} is the best algorithm, followed closely by another model-based algorithm, the \textit{GPR+SPEA2}. The third and fourth best results correspond to two metaheuristic algorithms, namely, \textit{NSGA-II} and \textit{OMOPSO}, respectively. Other model-based algorithms, like \textit{RF+NSGA-II} and \textit{RF+SPEA2} exhibit the fifth and sixth best performance. The two worse algorithms are \textit{GPR+NSGA-II} and \textit{GPR+Random}. Note, however, that \textit{GPR+Random}'s high variance is, once again, mainly due to its poorer performance on the second run. Conversely, the results obtained by \textit{GPR+NSGA-II} are relatively distant in the first and third runs (see \cref{sec:blackpavilionextra}).

Finally, we used \ac{HV} to circumvent some of the limitations inherent to other performance indicators, namely, the fact that they exclusively rely on a unique aspect. Contrastingly, \ac{HV} provides a measure of all three aspects. Regarding this indicator, three algorithms achieved the best result, including a metaheuristic, \textit{NSGA-II}, and two model-based algorithms, \textit{RF+NSGA-II} and \textit{RF+Random}. By observing the \ac{HV} column in \cref{table:blackpavilion}, it is possible to observe that all algorithms with the exception of \textit{OMOPSO} and \textit{GPR+Random} achieved similar results of \ac{HV}. Once more, the poor performance of \textit{GPR+Random} and \textit{OMOPSO} during the second run severely hindered their average performance.

%The analysis of the \ac{IGD} indicator revealed that the \textit{RF+Random} was the best algorithm. Surprisingly, the second, third, and fourth best algorithms belong to the metaheuristic class, namely, \textit{SMPSO}, \textit{NSGA-II}, and \textit{SPEA2}. These results can be explained by the overall spread of the associated Pareto fronts over the objective space covered by the combined Pareto front, as well as with a more uniform distribution across the objective space. In other words, while \textit{SPEA2} yields a reasonable number of optimal solutions, each covering regions close to the Pareto solutions' clusters in the combined Pareto front, \textit{GPR+SPEA2} yields a lower number of optimal solutions that fail to cover the different optimal regions identified in the combined Pareto front. Therefore, its \ac{IGD} value will be inherently lower, as \ac{IGD} measures how close the combined Pareto front is to the Pareto fronts identified by each algorithm. When considering the worst algorithms, \textit{GPR+NSGA-II} presented the worst performance, followed by \textit{GPR+Random}. Their lower performance resulted from the poor distribution of optimal solutions in the objective space, as well as from their generalized inability to discover solutions belonging to the combined Pareto front. 

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{Images/Evaluation/BlackPavilion/All_Algorithms_all_runs-2019-04-16.png}
	\caption[Black Pavilion: Pareto front plot]{Black Pavilion: Line plot of the Pareto fronts retrieved by each algorithm measuring the daylight conditions of the exhibition space as a function of the skylights' cost. These fronts are obtained by combining the values of the $3$ runs for each algorithm. The combined Pareto front is formed by finding the nondominated solutions from all the evaluated solutions.}
	\label{fig:blackpavilionallruns}
\end{figure}

\Cref{fig:blackpavilionallruns} presents the overall performance of all the tested algorithms after combining the results of the three runs. We can observe that, in general, all algorithms converged towards the \ac{cPF}, represented by the black points. Moreover, it is possible to observe that, overall, \textit{RF+Random}, \textit{RF+NSGA-II}, and \textit{NSGA-II} converged towards the \ac{cPF} with only a few solutions missing the \ac{cPF}. On the other hand, algorithms, like \textit{SPEA2}, \textit{SMPSO}, and \textit{GPR+Random} partially converged to the \ac{cPF}, however, for values of cost above $5500$€ these algorithms begin to deviate from the \ac{cPF}.

\subsection{Final Remarks}
In this chapter, we studied the application of different optimization algorithms to three \ac{BPO} case studies. To that end, we applied the methodology described in \cref{chap:implement}, which uses the Khepri \ac{AD} tool and the optimization framework discussed in \cref{chap:architecture}. We also used the analysis tools Radiance and Robot to measure the daylight and structural aspects of the different designs generated during the optimization.

The first case study belongs to a constrained \ac{SOO} problem, which involved the optimization of the daylight performance of a private house. We tackled this problem in two different ways. On the first stage, we followed a simple experimental approach, where we applied two sampling algorithms. On the second stage, we followed an \ac{SOO} approach, for which we have tested $13$ \acp{SOO} derivative-free algorithms, including $5$ direct-search, $5$ model-based, and $3$ metaheuristics. Results showed that, on average, global model-based algorithms exhibit best performance both in earlier and final stages of the optimization process. The same does not happen with metaheuristics that, although exhibiting a reasonable performance in earlier stages of the design, even surpassing the performance of global direct-search algorithms, stagnate shortly after. Conversely, global direct-search algorithms performed reasonably well, excelling metaheuristics shortly after $20$ evaluations. Moreover, we proved that both local direct-search and model-based algorithms can be fast solvers when provided with reasonable initial solutions. Indeed, two local algorithms converged towards optimal solutions after just $8$ evaluations. Given that each function evaluation took approximately $7$ minutes to complete, this evaluation difference becomes crucial for computationally complex optimization processes.

The second case study comprises a constrained bi-objective optimization problem of an arc-shaped space frame, whose objectives were the structural and the aesthetic aspects of the space frame. To that end, we evaluated the performance of $19$ \acp{MOOA}, $10$ metaheuristics, and $9$ model-based. Given the higher dimensionality of the algorithms' results, we measured the algorithms' performance in terms of the cardinality, diversity, and convergence of the retrieved results. The difficulties underlying \acp{MOOA} quality measurements prevailed during this case study. To circumvent this limitation, we evaluated the algorithms performance through a combination of \ac{MOO} indicators and line plots of the Pareto fronts returned by each algorithm. The results showed that \ac{PSO} metaheuristics algorithms achieved the overall best performance, whereas \ac{ES} metaheuristics algorithms achieved the overall worst performance. Moreover, while model-based algorithms did not outperform any of the metaheuristics algorithms, results suggest that model-based algorithms exploiting \ac{PSO} search strategy outperform other model-based variants, involving \textit{NSGA-II} or \textit{Random} strategies. Additionally, model-based algorithms depending on \textit{Random} search strategies seem to perform worse than other variants. 

The third and final case study also comprised a constrained bi-objective optimization of the cost and daylight aspects of an art exhibition space. To that end, we tested $10$ \acp{MOOA}, $4$ metaheuristics and $6$ model-based. Similarly to the previous case study, we also used a combination of Pareto front line plots with a set of \ac{MOOA} performance indicators to measure the quality of the optimization results. Results show that, contrastingly to the previous problem, \ac{PSO}-based algorithms fell short in terms of performance. Instead, \textit{RF+Random}, a model-based algorithm, was one of the top performers, followed by \textit{NSGA-II}, a metaheuristic algorithm. 

A careful analysis over the addressed \ac{MOO} problems reinforces the lack of consensus regarding the best way to assess the quality of \ac{MOOA}. From our studies, we conclude that their comparison should involve the computation of different \ac{MOO} indicators and the visualization of the Pareto front plots. %, as well as some critical thinking. 

Moreover, a thorough analysis of the case studies evidences that there is no single class, subclass, or algorithm that outperforms all others. This corroborates the ideas of Wolpert's optimization \ac{NFLT} \cite{Wolpert1997NFLT}. In the first problem, although global model-based algorithms achieve better results when no information is known, direct-search local algorithms are also able to achieve close to optimal results in fewer iterations. Moreover, while we did not test direct-search algorithms in a \ac{MOO} context problem, it is possible to visualize that, even between each category of derivative-free algorithms, %either metaheuristic or model-based, 
different subclasses present significantly different outcomes according to each problem's characteristics. For this reason, we conclude that prior to the selection of an optimization algorithm, users should test different algorithms for a fixed number of evaluations or a fixed amount of time. The proper algorithm selection can have beneficial impacts on the performance of an optimization process, namely in the time and quality of obtained results. In fact, not only can users select the most promising algorithm, but they can also explore the already evaluated solutions to hot-start local algorithms or to create an initial surrogate model. This surrogate model can be explored to determine an approximated behavior of the objective function or to perform optimization. 

Finally, another interesting conclusion is related to the impact of model-based algorithms in \ac{MOO} problems. Although model-based algorithms were not particularly successful in the second case study, they attained relatively better performance in the third case. More comparative \ac{MOO} studies are necessary, especially in \ac{BPO} problems, where studies combining model-based algorithms with Pareto-based optimization approaches are rare. In this vein, this dissertation contributes to bridge this gap and, in the next chapter, we outline future paths of research to further enrich these studies.